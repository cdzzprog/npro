{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'E:\\数据集\\cifar-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 训练样本 : 50000\n",
      "# 类别 : 10\n"
     ]
    }
   ],
   "source": [
    "def read_csv_labels(fname):\n",
    "    \"\"\"读取fname来给标签字典返回一个文件名\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # 跳过文件头行(列名)\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "print('# 训练样本 :', len(labels))\n",
    "print('# 类别 :', len(set(labels.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def copyfile(filename, target_dir):\n",
    "    \"\"\"将文件复制到目标目录\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    shutil.copy(filename, target_dir)\n",
    "\n",
    "#@save\n",
    "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
    "    \"\"\"将验证集从原始的训练集中拆分出来\"\"\"\n",
    "    # 训练数据集中样本最少的类别中的样本数\n",
    "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
    "    # 验证集中每个类别的样本数\n",
    "    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        fname = os.path.join(data_dir, 'train', train_file)\n",
    "        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                     'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                         'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                         'train', label))\n",
    "    return n_valid_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def reorg_test(data_dir):\n",
    "    \"\"\"在预测期间整理测试集，以方便读取\"\"\"\n",
    "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "        copyfile(os.path.join(data_dir, 'test', test_file),\n",
    "                 os.path.join(data_dir, 'train_valid_test', 'test',\n",
    "                              'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_cifar10_data(data_dir, valid_ratio):\n",
    "    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    reorg_test(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "valid_ratio = 0.1\n",
    "reorg_cifar10_data(data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    # 在高度和宽度上将图像放大到40像素的正方形\n",
    "    torchvision.transforms.Resize(40),\n",
    "    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n",
    "    # 生成一个面积为原始图像面积0.64～1倍的小正方形，\n",
    "    # 然后将其缩放为高度和宽度均为32像素的正方形\n",
    "    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                                   ratio=(1.0, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "\n",
    "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
    "                                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    num_classes = 10\n",
    "    net = d2l.resnet18(num_classes, 3)\n",
    "    return net\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
    "                              weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = d2l.train_batch_ch13(net, features, labels,\n",
    "                                          loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
    "                              None))\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\n",
    "            animator.add(epoch + 1, (None, None, valid_acc))\n",
    "        scheduler.step()\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.154, train acc 0.946, valid acc 0.857\n",
      "721.2 examples/sec on [device(type='cuda', index=0)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-08-24T15:01:52.262480</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 238.965625 183.35625 \n",
       "L 238.965625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 145.8 \n",
       "L 225.403125 145.8 \n",
       "L 225.403125 7.2 \n",
       "L 30.103125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 71.218914 145.8 \n",
       "L 71.218914 7.2 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m8ce97c4056\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ce97c4056\" x=\"71.218914\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(68.037664 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 122.613651 145.8 \n",
       "L 122.613651 7.2 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ce97c4056\" x=\"122.613651\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(116.251151 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 174.008388 145.8 \n",
       "L 174.008388 7.2 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ce97c4056\" x=\"174.008388\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(167.645888 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 225.403125 145.8 \n",
       "L 225.403125 7.2 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ce97c4056\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 30.103125 113.839376 \n",
       "L 225.403125 113.839376 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"m9b8be53bb9\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9b8be53bb9\" x=\"30.103125\" y=\"113.839376\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(7.2 117.638595) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 30.103125 77.374873 \n",
       "L 225.403125 77.374873 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9b8be53bb9\" x=\"30.103125\" y=\"77.374873\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(7.2 81.174092) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 30.103125 40.91037 \n",
       "L 225.403125 40.91037 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9b8be53bb9\" x=\"30.103125\" y=\"40.91037\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(7.2 44.709588) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 21.878505 13.5 \n",
       "L 23.932832 24.476133 \n",
       "L 25.98716 31.296363 \n",
       "L 28.041487 37.225138 \n",
       "L 30.095814 42.835629 \n",
       "L 30.103125 42.87379 \n",
       "L 32.157452 72.996281 \n",
       "L 34.21178 75.301833 \n",
       "L 36.266107 77.735215 \n",
       "L 38.320434 79.746975 \n",
       "L 40.374762 81.402695 \n",
       "L 40.382072 81.396143 \n",
       "L 42.4364 95.173304 \n",
       "L 44.490727 95.772599 \n",
       "L 46.545054 95.807794 \n",
       "L 48.599382 96.404789 \n",
       "L 50.653709 97.25763 \n",
       "L 50.66102 97.261028 \n",
       "L 52.715347 104.395841 \n",
       "L 54.769674 104.534091 \n",
       "L 56.824002 104.177771 \n",
       "L 58.878329 104.521234 \n",
       "L 60.932656 104.728563 \n",
       "L 60.939967 104.734994 \n",
       "L 62.994294 111.621567 \n",
       "L 65.048622 111.451626 \n",
       "L 67.102949 111.144053 \n",
       "L 69.157276 111.081223 \n",
       "L 71.211604 111.077097 \n",
       "L 71.218914 111.077602 \n",
       "L 73.273242 114.885324 \n",
       "L 75.327569 115.395392 \n",
       "L 77.381896 115.342778 \n",
       "L 79.436224 115.419165 \n",
       "L 81.490551 115.24778 \n",
       "L 81.497862 115.242462 \n",
       "L 83.552189 118.702703 \n",
       "L 85.606516 118.865036 \n",
       "L 87.660844 118.517308 \n",
       "L 89.715171 118.64568 \n",
       "L 91.769498 118.670359 \n",
       "L 91.776809 118.664842 \n",
       "L 93.831137 121.819688 \n",
       "L 95.885464 121.712278 \n",
       "L 97.939791 121.240413 \n",
       "L 99.994118 121.299721 \n",
       "L 102.048446 121.462363 \n",
       "L 102.055757 121.446613 \n",
       "L 104.110084 124.583907 \n",
       "L 106.164411 124.893231 \n",
       "L 108.218739 124.7696 \n",
       "L 110.273066 124.688439 \n",
       "L 112.327393 124.589361 \n",
       "L 112.334704 124.590793 \n",
       "L 114.389031 126.899553 \n",
       "L 116.443359 126.321694 \n",
       "L 118.497686 126.50626 \n",
       "L 120.552013 126.696435 \n",
       "L 122.606341 126.551597 \n",
       "L 122.613651 126.560043 \n",
       "L 124.667979 128.900464 \n",
       "L 126.722306 128.266621 \n",
       "L 128.776633 128.157617 \n",
       "L 130.830961 128.074691 \n",
       "L 132.885288 127.860344 \n",
       "L 132.892599 127.870894 \n",
       "L 134.946926 130.787271 \n",
       "L 137.001253 130.710431 \n",
       "L 139.055581 130.177606 \n",
       "L 141.109908 129.953307 \n",
       "L 143.164235 129.695038 \n",
       "L 143.171546 129.698892 \n",
       "L 145.225873 133.03344 \n",
       "L 147.280201 132.970132 \n",
       "L 149.334528 133.02151 \n",
       "L 151.388855 132.53622 \n",
       "L 153.443183 132.577472 \n",
       "L 153.450493 132.582448 \n",
       "L 155.504821 134.704381 \n",
       "L 157.559148 134.06908 \n",
       "L 159.613475 133.865218 \n",
       "L 161.667803 133.517085 \n",
       "L 163.72213 133.413215 \n",
       "L 163.729441 133.418846 \n",
       "L 165.783768 136.006679 \n",
       "L 167.838095 135.475796 \n",
       "L 169.892423 135.005516 \n",
       "L 171.94675 134.781371 \n",
       "L 174.001077 134.38151 \n",
       "L 174.008388 134.382756 \n",
       "L 176.062715 135.305459 \n",
       "L 178.117043 135.60883 \n",
       "L 180.17137 135.414311 \n",
       "L 182.225697 135.453568 \n",
       "L 184.280025 135.358156 \n",
       "L 184.287336 135.35474 \n",
       "L 186.341663 137.703202 \n",
       "L 188.39599 137.089486 \n",
       "L 190.450317 137.25227 \n",
       "L 192.504645 137.249673 \n",
       "L 194.558972 136.932975 \n",
       "L 194.566283 136.92908 \n",
       "L 196.62061 137.415574 \n",
       "L 198.674938 137.557493 \n",
       "L 200.729265 137.465663 \n",
       "L 202.783592 137.567892 \n",
       "L 204.837919 137.612174 \n",
       "L 204.84523 137.608119 \n",
       "L 206.899558 139.381058 \n",
       "L 208.953885 138.726159 \n",
       "L 211.008212 138.940617 \n",
       "L 213.06254 138.774493 \n",
       "L 215.116867 138.68909 \n",
       "L 215.124178 138.676217 \n",
       "L 217.178505 139.120777 \n",
       "L 219.232832 139.5 \n",
       "L 221.28716 139.386642 \n",
       "L 223.341487 139.189237 \n",
       "L 225.395814 139.033929 \n",
       "L 225.403125 139.038812 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 21.878505 128.024522 \n",
       "L 23.932832 123.85576 \n",
       "L 25.98716 121.211759 \n",
       "L 28.041487 118.811071 \n",
       "L 30.095814 116.57097 \n",
       "L 30.103125 116.554438 \n",
       "L 32.157452 104.609704 \n",
       "L 34.21178 104.094692 \n",
       "L 36.266107 103.082239 \n",
       "L 38.320434 102.28201 \n",
       "L 40.374762 101.696437 \n",
       "L 40.382072 101.700211 \n",
       "L 42.4364 96.775027 \n",
       "L 44.490727 96.324898 \n",
       "L 46.545054 96.312732 \n",
       "L 48.599382 96.138358 \n",
       "L 50.653709 95.780688 \n",
       "L 50.66102 95.783806 \n",
       "L 52.715347 93.668731 \n",
       "L 54.769674 93.417308 \n",
       "L 56.824002 93.447046 \n",
       "L 58.878329 93.317955 \n",
       "L 60.932656 93.243745 \n",
       "L 60.939967 93.242184 \n",
       "L 62.994294 91.000399 \n",
       "L 65.048622 91.069338 \n",
       "L 67.102949 91.040951 \n",
       "L 69.157276 91.038924 \n",
       "L 71.211604 91.027974 \n",
       "L 71.218914 91.02961 \n",
       "L 73.273242 89.791945 \n",
       "L 75.327569 89.491859 \n",
       "L 77.381896 89.570259 \n",
       "L 79.436224 89.493886 \n",
       "L 81.490551 89.486992 \n",
       "L 81.497862 89.491345 \n",
       "L 83.552189 88.267183 \n",
       "L 85.606516 88.218521 \n",
       "L 87.660844 88.27259 \n",
       "L 89.715171 88.277321 \n",
       "L 91.769498 88.276916 \n",
       "L 91.776809 88.280508 \n",
       "L 93.831137 87.229048 \n",
       "L 95.885464 87.318263 \n",
       "L 97.939791 87.45614 \n",
       "L 99.994118 87.454112 \n",
       "L 102.048446 87.443163 \n",
       "L 102.055757 87.445728 \n",
       "L 104.110084 86.117919 \n",
       "L 106.164411 86.174692 \n",
       "L 108.218739 86.290941 \n",
       "L 110.273066 86.324735 \n",
       "L 112.327393 86.369342 \n",
       "L 112.334704 86.369429 \n",
       "L 114.389031 85.517747 \n",
       "L 116.443359 85.736728 \n",
       "L 118.497686 85.771874 \n",
       "L 120.552013 85.740784 \n",
       "L 122.606341 85.801612 \n",
       "L 122.613651 85.797239 \n",
       "L 124.667979 84.917575 \n",
       "L 126.722306 85.144667 \n",
       "L 128.776633 85.136556 \n",
       "L 130.830961 85.187247 \n",
       "L 132.885288 85.227393 \n",
       "L 132.892599 85.221808 \n",
       "L 134.946926 83.928102 \n",
       "L 137.001253 84.159249 \n",
       "L 139.055581 84.317403 \n",
       "L 141.109908 84.422838 \n",
       "L 143.164235 84.458524 \n",
       "L 143.171546 84.458349 \n",
       "L 145.225873 83.473918 \n",
       "L 147.280201 83.283323 \n",
       "L 149.334528 83.279267 \n",
       "L 151.388855 83.486084 \n",
       "L 153.443183 83.512848 \n",
       "L 153.450493 83.511724 \n",
       "L 155.504821 82.735869 \n",
       "L 157.559148 83.068396 \n",
       "L 159.613475 83.138687 \n",
       "L 161.667803 83.287378 \n",
       "L 163.72213 83.370104 \n",
       "L 163.729441 83.370703 \n",
       "L 165.783768 82.541218 \n",
       "L 167.838095 82.67504 \n",
       "L 169.892423 82.825083 \n",
       "L 171.94675 82.847387 \n",
       "L 174.001077 82.964583 \n",
       "L 174.008388 82.963849 \n",
       "L 176.062715 82.403341 \n",
       "L 178.117043 82.318181 \n",
       "L 180.17137 82.460114 \n",
       "L 182.225697 82.474307 \n",
       "L 184.280025 82.469846 \n",
       "L 184.287336 82.471085 \n",
       "L 186.341663 81.722065 \n",
       "L 188.39599 81.953212 \n",
       "L 190.450317 81.905901 \n",
       "L 192.504645 81.882246 \n",
       "L 194.558972 82.041616 \n",
       "L 194.566283 82.043159 \n",
       "L 196.62061 81.843721 \n",
       "L 198.674938 81.896439 \n",
       "L 200.729265 81.843721 \n",
       "L 202.783592 81.788976 \n",
       "L 204.837919 81.793436 \n",
       "L 204.84523 81.796777 \n",
       "L 206.899558 81.25977 \n",
       "L 208.953885 81.438199 \n",
       "L 211.008212 81.327357 \n",
       "L 213.06254 81.379399 \n",
       "L 215.116867 81.425223 \n",
       "L 215.124178 81.425584 \n",
       "L 217.178505 81.348985 \n",
       "L 219.232832 81.255715 \n",
       "L 221.28716 81.273287 \n",
       "L 223.341487 81.316543 \n",
       "L 225.395814 81.34574 \n",
       "L 225.403125 81.344537 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 30.103125 108.521636 \n",
       "L 40.382072 104.825514 \n",
       "L 50.66102 103.262333 \n",
       "L 60.939967 97.11187 \n",
       "L 71.218914 95.037367 \n",
       "L 81.497862 94.175425 \n",
       "L 91.776809 94.511436 \n",
       "L 102.055757 93.547231 \n",
       "L 112.334704 92.217796 \n",
       "L 122.613651 89.66119 \n",
       "L 132.892599 91.633429 \n",
       "L 143.171546 90.683832 \n",
       "L 153.450493 89.66119 \n",
       "L 163.729441 89.690408 \n",
       "L 174.008388 89.763454 \n",
       "L 184.287336 88.112617 \n",
       "L 194.566283 89.31057 \n",
       "L 204.84523 88.024962 \n",
       "L 215.124178 88.375582 \n",
       "L 225.403125 87.805824 \n",
       "\" clip-path=\"url(#pae317c53c3)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 145.8 \n",
       "L 30.103125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 225.403125 145.8 \n",
       "L 225.403125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 145.8 \n",
       "L 225.403125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.2 \n",
       "L 225.403125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 140.634375 59.234375 \n",
       "L 218.403125 59.234375 \n",
       "Q 220.403125 59.234375 220.403125 57.234375 \n",
       "L 220.403125 14.2 \n",
       "Q 220.403125 12.2 218.403125 12.2 \n",
       "L 140.634375 12.2 \n",
       "Q 138.634375 12.2 138.634375 14.2 \n",
       "L 138.634375 57.234375 \n",
       "Q 138.634375 59.234375 140.634375 59.234375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 142.634375 20.298438 \n",
       "L 152.634375 20.298438 \n",
       "L 162.634375 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(170.634375 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 142.634375 34.976563 \n",
       "L 152.634375 34.976563 \n",
       "L 162.634375 34.976563 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(170.634375 38.476563) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\">\n",
       "     <path d=\"M 142.634375 49.654688 \n",
       "L 152.634375 49.654688 \n",
       "L 162.634375 49.654688 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- valid acc -->\n",
       "     <g transform=\"translate(170.634375 53.154688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"239.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"271.289062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"332.568359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"387.548828\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pae317c53c3\">\n",
       "   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4\n",
    "lr_period, lr_decay, net = 4, 0.9, get_net()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "毕节滑坡的语义分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # 编码器\n",
    "        self.enc1 = self.conv_block(3, 64)   # 输入通道数调整为 3\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.enc5 = self.conv_block(512, 1024)\n",
    "\n",
    "        # 解码器\n",
    "        self.upconv6 = self.upconv_block(1024, 512)\n",
    "        self.upconv7 = self.upconv_block(512, 256)\n",
    "        self.upconv8 = self.upconv_block(256, 128)\n",
    "        self.upconv9 = self.upconv_block(128, 64)\n",
    "\n",
    "        # 合并操作\n",
    "        self.merge6 = self.conv_block(1024, 512, batch_norm=False)\n",
    "        self.merge7 = self.conv_block(512, 256, batch_norm=False)\n",
    "        self.merge8 = self.conv_block(256, 128, batch_norm=False)\n",
    "        self.merge9 = self.conv_block(128, 64, batch_norm=False)\n",
    "\n",
    "        # 输出层\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, batch_norm=True):\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            layers.insert(2, nn.BatchNorm2d(out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n",
    "        enc5 = self.enc5(F.max_pool2d(enc4, 2))\n",
    "\n",
    "        # 解码\n",
    "        up6 = self.upconv6(enc5)\n",
    "        merge6 = torch.cat([up6, enc4], dim=1)\n",
    "        conv6 = self.merge6(merge6)\n",
    "\n",
    "        up7 = self.upconv7(conv6)\n",
    "        merge7 = torch.cat([up7, enc3], dim=1)\n",
    "        conv7 = self.merge7(merge7)\n",
    "\n",
    "        up8 = self.upconv8(conv7)\n",
    "        merge8 = torch.cat([up8, enc2], dim=1)\n",
    "        conv8 = self.merge8(merge8)\n",
    "\n",
    "        up9 = self.upconv9(conv8)\n",
    "        merge9 = torch.cat([up9, enc1], dim=1)\n",
    "        conv9 = self.merge9(merge9)\n",
    "\n",
    "        outputs = self.final(conv9)\n",
    "        return outputs\n",
    "\n",
    "# 测试模型的定义\n",
    "if __name__ == \"__main__\":\n",
    "    model = UNet()\n",
    "    x = torch.randn(4, 3, 256, 256)  # 创建一个形状为 (4, 3, 256, 256) 的输入张量\n",
    "    output = model(x)\n",
    "    print(output.shape)  # 应该打印 (4, 1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.py\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LandslideDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.images=os.listdir(image_dir)\n",
    "\n",
    "        # self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png') or f.endswith('.jpg')])\n",
    "        # self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png') or f.endswith('.jpg')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[index])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[index])\n",
    "        \n",
    "        image= Image.open(image_path).convert('RGB')  # 转换为RGB模式\n",
    "        mask = Image.open(label_path).convert('L')  # 转换为灰度模式\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            augmentation=self.transform(iamge=image,mask=mask)\n",
    "            image=augmentation(\"image\")\n",
    "            mask=augmentation(\"mask\")\n",
    "            \n",
    "        return image,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import  tqdm\n",
    "import random\n",
    "\n",
    "#hyperparms\n",
    "learning_rate=1e-4\n",
    "batch_size=4\n",
    "num_epochs=2\n",
    "num_workers=2\n",
    "pin_memory=False\n",
    "load_model=True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\龙儿璨\\AppData\\Local\\Temp\\ipykernel_27064\\209780630.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = [torch.tensor(image) for image in images]\n",
      "C:\\Users\\龙儿璨\\AppData\\Local\\Temp\\ipykernel_27064\\209780630.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(label) for label in labels]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.3238\n",
      "Epoch [2/300], Loss: 0.1783\n",
      "Epoch [3/300], Loss: 0.1572\n",
      "Epoch [4/300], Loss: 0.1462\n",
      "Epoch [5/300], Loss: 0.1309\n",
      "Epoch [6/300], Loss: 0.1282\n",
      "Epoch [7/300], Loss: 0.1246\n",
      "Epoch [8/300], Loss: 0.1198\n",
      "Epoch [9/300], Loss: 0.1167\n",
      "Epoch [10/300], Loss: 0.1164\n",
      "Epoch [11/300], Loss: 0.1103\n",
      "Epoch [12/300], Loss: 0.1108\n",
      "Epoch [13/300], Loss: 0.1063\n",
      "Epoch [14/300], Loss: 0.1043\n",
      "Epoch [15/300], Loss: 0.1012\n",
      "Epoch [16/300], Loss: 0.0989\n",
      "Epoch [17/300], Loss: 0.0942\n",
      "Epoch [18/300], Loss: 0.0955\n",
      "Epoch [19/300], Loss: 0.0957\n",
      "Epoch [20/300], Loss: 0.1013\n",
      "Epoch [21/300], Loss: 0.0912\n",
      "Epoch [22/300], Loss: 0.0940\n",
      "Epoch [23/300], Loss: 0.0901\n",
      "Epoch [24/300], Loss: 0.0908\n",
      "Epoch [25/300], Loss: 0.0886\n",
      "Epoch [26/300], Loss: 0.0869\n",
      "Epoch [27/300], Loss: 0.0862\n",
      "Epoch [28/300], Loss: 0.0859\n",
      "Epoch [29/300], Loss: 0.0851\n",
      "Epoch [30/300], Loss: 0.0822\n",
      "Epoch [31/300], Loss: 0.0805\n",
      "Epoch [32/300], Loss: 0.0824\n",
      "Epoch [33/300], Loss: 0.0810\n",
      "Epoch [34/300], Loss: 0.0800\n",
      "Epoch [35/300], Loss: 0.0781\n",
      "Epoch [36/300], Loss: 0.0789\n",
      "Epoch [37/300], Loss: 0.0797\n",
      "Epoch [38/300], Loss: 0.0754\n",
      "Epoch [39/300], Loss: 0.0767\n",
      "Epoch [40/300], Loss: 0.0774\n",
      "Epoch [41/300], Loss: 0.0756\n",
      "Epoch [42/300], Loss: 0.0727\n",
      "Epoch [43/300], Loss: 0.0710\n",
      "Epoch [44/300], Loss: 0.0716\n",
      "Epoch [45/300], Loss: 0.0706\n",
      "Epoch [46/300], Loss: 0.0706\n",
      "Epoch [47/300], Loss: 0.0686\n",
      "Epoch [48/300], Loss: 0.0680\n",
      "Epoch [49/300], Loss: 0.0693\n",
      "Epoch [50/300], Loss: 0.0684\n",
      "Epoch [51/300], Loss: 0.0646\n",
      "Epoch [52/300], Loss: 0.0650\n",
      "Epoch [53/300], Loss: 0.0652\n",
      "Epoch [54/300], Loss: 0.0633\n",
      "Epoch [55/300], Loss: 0.0678\n",
      "Epoch [56/300], Loss: 0.0622\n",
      "Epoch [57/300], Loss: 0.0634\n",
      "Epoch [58/300], Loss: 0.0617\n",
      "Epoch [59/300], Loss: 0.0585\n",
      "Epoch [60/300], Loss: 0.0623\n",
      "Epoch [61/300], Loss: 0.0597\n",
      "Epoch [62/300], Loss: 0.0611\n",
      "Epoch [63/300], Loss: 0.0594\n",
      "Epoch [64/300], Loss: 0.0571\n",
      "Epoch [65/300], Loss: 0.0566\n",
      "Epoch [66/300], Loss: 0.0602\n",
      "Epoch [67/300], Loss: 0.0576\n",
      "Epoch [68/300], Loss: 0.0557\n",
      "Epoch [69/300], Loss: 0.0567\n",
      "Epoch [70/300], Loss: 0.0548\n",
      "Epoch [71/300], Loss: 0.0545\n",
      "Epoch [72/300], Loss: 0.0532\n",
      "Epoch [73/300], Loss: 0.0515\n",
      "Epoch [74/300], Loss: 0.0547\n",
      "Epoch [75/300], Loss: 0.0521\n",
      "Epoch [76/300], Loss: 0.0560\n",
      "Epoch [77/300], Loss: 0.0533\n",
      "Epoch [78/300], Loss: 0.0489\n",
      "Epoch [79/300], Loss: 0.0496\n",
      "Epoch [80/300], Loss: 0.0509\n",
      "Epoch [81/300], Loss: 0.0516\n",
      "Epoch [82/300], Loss: 0.0494\n",
      "Epoch [83/300], Loss: 0.0489\n",
      "Epoch [84/300], Loss: 0.0478\n",
      "Epoch [85/300], Loss: 0.0470\n",
      "Epoch [86/300], Loss: 0.0477\n",
      "Epoch [87/300], Loss: 0.0472\n",
      "Epoch [88/300], Loss: 0.0465\n",
      "Epoch [89/300], Loss: 0.0484\n",
      "Epoch [90/300], Loss: 0.0473\n",
      "Epoch [91/300], Loss: 0.0445\n",
      "Epoch [92/300], Loss: 0.0444\n",
      "Epoch [93/300], Loss: 0.0454\n",
      "Epoch [94/300], Loss: 0.0467\n",
      "Epoch [95/300], Loss: 0.0426\n",
      "Epoch [96/300], Loss: 0.0422\n",
      "Epoch [97/300], Loss: 0.0449\n",
      "Epoch [98/300], Loss: 0.0420\n",
      "Epoch [99/300], Loss: 0.0455\n",
      "Epoch [100/300], Loss: 0.0399\n",
      "Epoch [101/300], Loss: 0.0440\n",
      "Epoch [102/300], Loss: 0.0420\n",
      "Epoch [103/300], Loss: 0.0425\n",
      "Epoch [104/300], Loss: 0.0405\n",
      "Epoch [105/300], Loss: 0.0394\n",
      "Epoch [106/300], Loss: 0.0422\n",
      "Epoch [107/300], Loss: 0.0394\n",
      "Epoch [108/300], Loss: 0.0399\n",
      "Epoch [109/300], Loss: 0.0394\n",
      "Epoch [110/300], Loss: 0.0463\n",
      "Epoch [111/300], Loss: 0.0407\n",
      "Epoch [112/300], Loss: 0.0383\n",
      "Epoch [113/300], Loss: 0.0372\n",
      "Epoch [114/300], Loss: 0.0383\n",
      "Epoch [115/300], Loss: 0.0496\n",
      "Epoch [116/300], Loss: 0.0366\n",
      "Epoch [117/300], Loss: 0.0373\n",
      "Epoch [118/300], Loss: 0.0366\n",
      "Epoch [119/300], Loss: 0.0376\n",
      "Epoch [120/300], Loss: 0.0376\n",
      "Epoch [121/300], Loss: 0.0396\n",
      "Epoch [122/300], Loss: 0.0408\n",
      "Epoch [123/300], Loss: 0.0369\n",
      "Epoch [124/300], Loss: 0.0339\n",
      "Epoch [125/300], Loss: 0.0375\n",
      "Epoch [126/300], Loss: 0.0359\n",
      "Epoch [127/300], Loss: 0.0383\n",
      "Epoch [128/300], Loss: 0.0355\n",
      "Epoch [129/300], Loss: 0.0354\n",
      "Epoch [130/300], Loss: 0.0336\n",
      "Epoch [131/300], Loss: 0.0372\n",
      "Epoch [132/300], Loss: 0.0384\n",
      "Epoch [133/300], Loss: 0.0361\n",
      "Epoch [134/300], Loss: 0.0333\n",
      "Epoch [135/300], Loss: 0.0342\n",
      "Epoch [136/300], Loss: 0.0356\n",
      "Epoch [137/300], Loss: 0.0333\n",
      "Epoch [138/300], Loss: 0.0344\n",
      "Epoch [139/300], Loss: 0.0356\n",
      "Epoch [140/300], Loss: 0.0369\n",
      "Epoch [141/300], Loss: 0.0350\n",
      "Epoch [142/300], Loss: 0.0326\n",
      "Epoch [143/300], Loss: 0.0324\n",
      "Epoch [144/300], Loss: 0.0352\n",
      "Epoch [145/300], Loss: 0.0312\n",
      "Epoch [146/300], Loss: 0.0370\n",
      "Epoch [147/300], Loss: 0.0318\n",
      "Epoch [148/300], Loss: 0.0302\n",
      "Epoch [149/300], Loss: 0.0451\n",
      "Epoch [150/300], Loss: 0.0327\n",
      "Epoch [151/300], Loss: 0.0302\n",
      "Epoch [152/300], Loss: 0.0321\n",
      "Epoch [153/300], Loss: 0.0313\n",
      "Epoch [154/300], Loss: 0.0318\n",
      "Epoch [155/300], Loss: 0.0307\n",
      "Epoch [156/300], Loss: 0.0375\n",
      "Epoch [157/300], Loss: 0.0320\n",
      "Epoch [158/300], Loss: 0.0316\n",
      "Epoch [159/300], Loss: 0.0321\n",
      "Epoch [160/300], Loss: 0.0336\n",
      "Epoch [161/300], Loss: 0.0337\n",
      "Epoch [162/300], Loss: 0.0309\n",
      "Epoch [163/300], Loss: 0.0288\n",
      "Epoch [164/300], Loss: 0.0307\n",
      "Epoch [165/300], Loss: 0.0322\n",
      "Epoch [166/300], Loss: 0.0300\n",
      "Epoch [167/300], Loss: 0.0299\n",
      "Epoch [168/300], Loss: 0.0327\n",
      "Epoch [169/300], Loss: 0.0306\n",
      "Epoch [170/300], Loss: 0.0286\n",
      "Epoch [171/300], Loss: 0.0292\n",
      "Epoch [172/300], Loss: 0.0328\n",
      "Epoch [173/300], Loss: 0.0360\n",
      "Epoch [174/300], Loss: 0.0283\n",
      "Epoch [175/300], Loss: 0.0309\n",
      "Epoch [176/300], Loss: 0.0282\n",
      "Epoch [177/300], Loss: 0.0300\n",
      "Epoch [178/300], Loss: 0.0310\n",
      "Epoch [179/300], Loss: 0.0310\n",
      "Epoch [180/300], Loss: 0.0286\n",
      "Epoch [181/300], Loss: 0.0317\n",
      "Epoch [182/300], Loss: 0.0284\n",
      "Epoch [183/300], Loss: 0.0287\n",
      "Epoch [184/300], Loss: 0.0289\n",
      "Epoch [185/300], Loss: 0.0334\n",
      "Epoch [186/300], Loss: 0.0345\n",
      "Epoch [187/300], Loss: 0.0283\n",
      "Epoch [188/300], Loss: 0.0268\n",
      "Epoch [189/300], Loss: 0.0280\n",
      "Epoch [190/300], Loss: 0.0362\n",
      "Epoch [191/300], Loss: 0.0293\n",
      "Epoch [192/300], Loss: 0.0263\n",
      "Epoch [193/300], Loss: 0.0267\n",
      "Epoch [194/300], Loss: 0.0332\n",
      "Epoch [195/300], Loss: 0.0335\n",
      "Epoch [196/300], Loss: 0.0284\n",
      "Epoch [197/300], Loss: 0.0259\n",
      "Epoch [198/300], Loss: 0.0298\n",
      "Epoch [199/300], Loss: 0.0283\n",
      "Epoch [200/300], Loss: 0.0281\n",
      "Epoch [201/300], Loss: 0.0301\n",
      "Epoch [202/300], Loss: 0.0263\n",
      "Epoch [203/300], Loss: 0.0275\n",
      "Epoch [204/300], Loss: 0.0339\n",
      "Epoch [205/300], Loss: 0.0275\n",
      "Epoch [206/300], Loss: 0.0259\n",
      "Epoch [207/300], Loss: 0.0290\n",
      "Epoch [208/300], Loss: 0.0264\n",
      "Epoch [209/300], Loss: 0.0258\n",
      "Epoch [210/300], Loss: 0.0286\n",
      "Epoch [211/300], Loss: 0.0284\n",
      "Epoch [212/300], Loss: 0.0266\n",
      "Epoch [213/300], Loss: 0.0276\n",
      "Epoch [214/300], Loss: 0.0275\n",
      "Epoch [215/300], Loss: 0.0296\n",
      "Epoch [216/300], Loss: 0.0264\n",
      "Epoch [217/300], Loss: 0.0307\n",
      "Epoch [218/300], Loss: 0.0256\n",
      "Epoch [219/300], Loss: 0.0265\n",
      "Epoch [220/300], Loss: 0.0277\n",
      "Epoch [221/300], Loss: 0.0301\n",
      "Epoch [222/300], Loss: 0.0259\n",
      "Epoch [223/300], Loss: 0.0272\n",
      "Epoch [224/300], Loss: 0.0293\n",
      "Epoch [225/300], Loss: 0.0266\n",
      "Epoch [226/300], Loss: 0.0262\n",
      "Epoch [227/300], Loss: 0.0273\n",
      "Epoch [228/300], Loss: 0.0322\n",
      "Epoch [229/300], Loss: 0.0251\n",
      "Epoch [230/300], Loss: 0.0235\n",
      "Epoch [231/300], Loss: 0.0257\n",
      "Epoch [232/300], Loss: 0.0353\n",
      "Epoch [233/300], Loss: 0.0321\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(input_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "        self.center = self.conv_block(256, 512)\n",
    "        self.dec4 = self.conv_block(512 + 256, 256)\n",
    "        self.dec3 = self.conv_block(256 + 128, 128)\n",
    "        self.dec2 = self.conv_block(128 + 64, 64)\n",
    "        self.dec1 = self.conv_block(64 + 32, 32)\n",
    "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        enc1 = self.dropout(self.enc1(x))\n",
    "        enc2 = self.dropout(self.enc2(self.pool(enc1)))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        center = self.center(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([enc4, self.crop_and_concat(self.up(center), enc4)], 1))\n",
    "        dec3 = self.dec3(torch.cat([enc3, self.crop_and_concat(self.up(dec4), enc3)], 1))\n",
    "        dec2 = self.dec2(torch.cat([enc2, self.crop_and_concat(self.up(dec3), enc2)], 1))\n",
    "        dec1 = self.dec1(torch.cat([enc1, self.crop_and_concat(self.up(dec2), enc1)], 1))\n",
    "        final = self.final(dec1).squeeze()\n",
    "\n",
    "        return final\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        # 计算要裁剪的边界\n",
    "        diffY = bypass.size()[2] - upsampled.size()[2]\n",
    "        diffX = bypass.size()[3] - upsampled.size()[3]\n",
    "\n",
    "        # 裁剪输入张量\n",
    "        upsampled = torch.nn.functional.pad(upsampled, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        return upsampled\n",
    "class RSDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images = self.read_multiband_images(images_dir)\n",
    "        self.labels = self.read_singleband_labels(labels_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "    def read_multiband_images(self, images_dir):\n",
    "        images = []\n",
    "        for image_file in os.listdir(images_dir):\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "            rsdl_data = gdal.Open(image_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open image file: {image_path}\")\n",
    "            images.append(np.stack([rsdl_data.GetRasterBand(i).ReadAsArray() for i in range(1, 4)], axis=0))\n",
    "        return images\n",
    "\n",
    "    def read_singleband_labels(self, labels_dir):\n",
    "        labels = []\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            rsdl_data = gdal.Open(label_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open label file: {label_path}\")\n",
    "            labels.append(rsdl_data.GetRasterBand(1).ReadAsArray())\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()  # Converts the image to a PyTorch tensor\n",
    "    ])\n",
    "def transform(image, label):\n",
    "    from PIL import Image\n",
    "    image = Image.fromarray(image)\n",
    "    label = Image.fromarray(label)\n",
    "\n",
    "    transform = get_transform()\n",
    "    image = transform(image)\n",
    "    label = transform(label)\n",
    "    \n",
    "    image = np.array(image)\n",
    "    label = np.array(label)\n",
    "    return image, label\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        images = [torch.tensor(image) for image in images]\n",
    "        labels = [torch.tensor(label) for label in labels]\n",
    "        \n",
    "        # 找到最大宽度和高度\n",
    "        max_width = max([img.shape[2] for img in images])\n",
    "        max_height = max([img.shape[1] for img in images])\n",
    "\n",
    "        padded_images = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for img, lbl in zip(images, labels):\n",
    "            # 填充图像和标签\n",
    "            pad_img = torch.zeros((img.shape[0], max_height, max_width))\n",
    "            pad_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "            pad_lbl = torch.zeros((max_height, max_width))\n",
    "            pad_lbl[:lbl.shape[0], :lbl.shape[1]] = lbl\n",
    "            padded_images.append(pad_img)\n",
    "            padded_labels.append(pad_lbl)\n",
    "        \n",
    "        return torch.stack(padded_images), torch.stack(padded_labels)\n",
    "\n",
    "images_dir = 'E:\\数据集\\山体滑坡数据集\\landslide\\image3/'\n",
    "labels_dir = 'E:\\数据集\\山体滑坡数据集\\landslide\\mask3/'\n",
    "\n",
    "transform = lambda img, lbl: transform(img, lbl)  # Apply the transformation\n",
    "dataset = RSDataset(images_dir, labels_dir, transform=transform)\n",
    "trainloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "model = UNet(3, 1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "# # 训练和测试代码\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for i, (images, labels) in enumerate(trainloader):\n",
    "#         images = images.float().to(device)\n",
    "#         labels = labels.float().to(device) / 255.0\n",
    "#         outputs = model(images)\n",
    "#         labels = labels.squeeze(0)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "\n",
    "# # 保存模型\n",
    "# torch.save(model.state_dict(), 'models_building_53.pth')\n",
    "# def evaluate_model(model, dataloader):\n",
    "#     model.eval()\n",
    "#     all_labels = []\n",
    "#     all_predictions = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in dataloader:\n",
    "#             images = images.float().to(device)\n",
    "#             labels = labels.float().to(device) / 255.0\n",
    "            \n",
    "#             outputs = model(images).squeeze().cpu().numpy()\n",
    "#             labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "#             all_predictions.extend(outputs.flatten())\n",
    "#             all_labels.extend(labels.flatten())\n",
    "\n",
    "#     return np.array(all_labels), np.array(all_predictions)\n",
    "# early_stop_count = 0\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 10  # 设置早停的耐心值\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device) / 255.0\n",
    "        outputs = model(images)\n",
    "        labels = labels.squeeze(0)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    torch.save(model.state_dict(), f'models_building_{epoch + 1}.pth')\n",
    "    # val_loss = evaluate_model_loss(model, val_loader)\n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     early_stop_count = 0\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')  # 保存最优模型\n",
    "    # else:\n",
    "    #     early_stop_count += 1\n",
    "    #     if early_stop_count >= patience:\n",
    "    #         print(\"Early stopping triggered.\")\n",
    "    #         break\n",
    "\n",
    "\n",
    "\n",
    "    # 计算验证集损失\n",
    "\n",
    "\n",
    "\n",
    "test_images_dir = 'E:\\数据集\\山体滑坡数据集\\landslide\\image1/'\n",
    "test_labels_dir = 'E:\\数据集\\山体滑坡数据集\\landslide\\mask1/'\n",
    "# 测试数据加载器\n",
    "test_transform = get_transform()\n",
    "test_dataset = RSDataset(test_images_dir, test_labels_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.float().to(device) / 255.0\n",
    "            \n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            all_predictions.extend(outputs.flatten())\n",
    "            all_labels.extend(labels.flatten())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    average_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall curve (AP={average_precision:.2f})')\n",
    "    plt.show()\n",
    "\n",
    "# 评估和绘制\n",
    "y_true, y_scores = evaluate_model(model, test_loader)\n",
    "plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\龙儿璨\\AppData\\Local\\Temp\\ipykernel_19452\\1219541759.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = [torch.tensor(image) for image in images]\n",
      "C:\\Users\\龙儿璨\\AppData\\Local\\Temp\\ipykernel_19452\\1219541759.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(label) for label in labels]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.1564\n",
      "Epoch [2/50], Loss: 0.0922\n",
      "Epoch [3/50], Loss: 0.0897\n",
      "Epoch [4/50], Loss: 0.0862\n",
      "Epoch [5/50], Loss: 0.0839\n",
      "Epoch [6/50], Loss: 0.0832\n",
      "Epoch [7/50], Loss: 0.0812\n",
      "Epoch [8/50], Loss: 0.0804\n",
      "Epoch [9/50], Loss: 0.0801\n",
      "Epoch [10/50], Loss: 0.0780\n",
      "Epoch [11/50], Loss: 0.0766\n",
      "Epoch [12/50], Loss: 0.0763\n",
      "Epoch [13/50], Loss: 0.0761\n",
      "Epoch [14/50], Loss: 0.0751\n",
      "Epoch [15/50], Loss: 0.0757\n",
      "Epoch [16/50], Loss: 0.0735\n",
      "Epoch [17/50], Loss: 0.0734\n",
      "Epoch [18/50], Loss: 0.0741\n",
      "Epoch [19/50], Loss: 0.0722\n",
      "Epoch [20/50], Loss: 0.0723\n",
      "Epoch [21/50], Loss: 0.0722\n",
      "Epoch [22/50], Loss: 0.0707\n",
      "Epoch [23/50], Loss: 0.0718\n",
      "Epoch [24/50], Loss: 0.0698\n",
      "Epoch [25/50], Loss: 0.0706\n",
      "Epoch [26/50], Loss: 0.0696\n",
      "Epoch [27/50], Loss: 0.0700\n",
      "Epoch [28/50], Loss: 0.0686\n",
      "Epoch [29/50], Loss: 0.0689\n",
      "Epoch [30/50], Loss: 0.0705\n",
      "Epoch [31/50], Loss: 0.0692\n",
      "Epoch [32/50], Loss: 0.0679\n",
      "Epoch [33/50], Loss: 0.0678\n",
      "Epoch [34/50], Loss: 0.0678\n",
      "Epoch [35/50], Loss: 0.0685\n",
      "Epoch [36/50], Loss: 0.0673\n",
      "Epoch [37/50], Loss: 0.0693\n",
      "Epoch [38/50], Loss: 0.0659\n",
      "Epoch [39/50], Loss: 0.0672\n",
      "Epoch [40/50], Loss: 0.0664\n",
      "Epoch [41/50], Loss: 0.0665\n",
      "Epoch [42/50], Loss: 0.0691\n",
      "Epoch [43/50], Loss: 0.0677\n",
      "Epoch [44/50], Loss: 0.0667\n",
      "Epoch [45/50], Loss: 0.0673\n",
      "Epoch [46/50], Loss: 0.0658\n",
      "Epoch [47/50], Loss: 0.0660\n",
      "Epoch [48/50], Loss: 0.0652\n",
      "Epoch [49/50], Loss: 0.0662\n",
      "Epoch [50/50], Loss: 0.0657\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(input_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "        self.center = self.conv_block(256, 512)\n",
    "        self.dec4 = self.conv_block(512 + 256, 256)\n",
    "        self.dec3 = self.conv_block(256 + 128, 128)\n",
    "        self.dec2 = self.conv_block(128 + 64, 64)\n",
    "        self.dec1 = self.conv_block(64 + 32, 32)\n",
    "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        enc1 = self.dropout(self.enc1(x))\n",
    "        enc2 = self.dropout(self.enc2(self.pool(enc1)))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        center = self.center(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([enc4, self.crop_and_concat(self.up(center), enc4)], 1))\n",
    "        dec3 = self.dec3(torch.cat([enc3, self.crop_and_concat(self.up(dec4), enc3)], 1))\n",
    "        dec2 = self.dec2(torch.cat([enc2, self.crop_and_concat(self.up(dec3), enc2)], 1))\n",
    "        dec1 = self.dec1(torch.cat([enc1, self.crop_and_concat(self.up(dec2), enc1)], 1))\n",
    "        final = self.final(dec1).squeeze()\n",
    "\n",
    "        return final\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        # 计算要裁剪的边界\n",
    "        diffY = bypass.size()[2] - upsampled.size()[2]\n",
    "        diffX = bypass.size()[3] - upsampled.size()[3]\n",
    "\n",
    "        # 裁剪输入张量\n",
    "        upsampled = torch.nn.functional.pad(upsampled, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        return upsampled\n",
    "\n",
    "\n",
    "class RSDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images = self.read_multiband_images(images_dir)\n",
    "        self.labels = self.read_singleband_labels(labels_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "    def read_multiband_images(self, images_dir):\n",
    "        images = []\n",
    "        for image_file in os.listdir(images_dir):\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "            rsdl_data = gdal.Open(image_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open image file: {image_path}\")\n",
    "            images.append(np.stack([rsdl_data.GetRasterBand(i).ReadAsArray() for i in range(1, 4)], axis=0))\n",
    "        return images\n",
    "\n",
    "    def read_singleband_labels(self, labels_dir):\n",
    "        labels = []\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            rsdl_data = gdal.Open(label_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open label file: {label_path}\")\n",
    "            labels.append(rsdl_data.GetRasterBand(1).ReadAsArray())\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()  # Converts the image to a PyTorch tensor\n",
    "    ])\n",
    "def transform(image, label):\n",
    "    from PIL import Image\n",
    "    image = Image.fromarray(image)\n",
    "    label = Image.fromarray(label)\n",
    "\n",
    "    transform = get_transform()\n",
    "    image = transform(image)\n",
    "    label = transform(label)\n",
    "    \n",
    "    image = np.array(image)\n",
    "    label = np.array(label)\n",
    "    return image, label\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        images = [torch.tensor(image) for image in images]\n",
    "        labels = [torch.tensor(label) for label in labels]\n",
    "        \n",
    "        # 找到最大宽度和高度\n",
    "        max_width = max([img.shape[2] for img in images])\n",
    "        max_height = max([img.shape[1] for img in images])\n",
    "\n",
    "        padded_images = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for img, lbl in zip(images, labels):\n",
    "            # 填充图像和标签\n",
    "            pad_img = torch.zeros((img.shape[0], max_height, max_width))\n",
    "            pad_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "            pad_lbl = torch.zeros((max_height, max_width))\n",
    "            pad_lbl[:lbl.shape[0], :lbl.shape[1]] = lbl\n",
    "            padded_images.append(pad_img)\n",
    "            padded_labels.append(pad_lbl)\n",
    "        \n",
    "        return torch.stack(padded_images), torch.stack(padded_labels)\n",
    "\n",
    "images_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\train\\\\images/'\n",
    "labels_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\train\\\\masks'\n",
    "\n",
    "transform = lambda img, lbl: transform(img, lbl)  # Apply the transformation\n",
    "dataset = RSDataset(images_dir, labels_dir, transform=transform)\n",
    "trainloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "model = UNet(3, 1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device) / 255.0\n",
    "        outputs = model(images)\n",
    "        labels = labels.squeeze(0)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    torch.save(model.state_dict(), f'models_building_{epoch + 1}.pth')\n",
    "\n",
    "\n",
    "\n",
    "test_images_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\validation\\\\images/'\n",
    "test_labels_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\validation\\\\masks/'\n",
    "# 测试数据加载器\n",
    "test_transform = get_transform()\n",
    "test_dataset = RSDataset(test_images_dir, test_labels_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.float().to(device) / 255.0\n",
    "            \n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            all_predictions.extend(outputs.flatten())\n",
    "            all_labels.extend(labels.flatten())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    average_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall curve (AP={average_precision:.2f})')\n",
    "    plt.show()\n",
    "\n",
    "# 评估和绘制\n",
    "y_true, y_scores = evaluate_model(model, test_loader)\n",
    "plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.1652\n",
      "Epoch [2/50], Loss: 0.0923\n",
      "Epoch [3/50], Loss: 0.0890\n",
      "Epoch [4/50], Loss: 0.0868\n",
      "Epoch [5/50], Loss: 0.0846\n",
      "Epoch [6/50], Loss: 0.0842\n",
      "Epoch [7/50], Loss: 0.0803\n",
      "Epoch [8/50], Loss: 0.0806\n",
      "Epoch [9/50], Loss: 0.0790\n",
      "Epoch [10/50], Loss: 0.0783\n",
      "Epoch [11/50], Loss: 0.0768\n",
      "Epoch [12/50], Loss: 0.0775\n",
      "Epoch [13/50], Loss: 0.0771\n",
      "Epoch [14/50], Loss: 0.0759\n",
      "Epoch [15/50], Loss: 0.0743\n",
      "Epoch [16/50], Loss: 0.0750\n",
      "Epoch [17/50], Loss: 0.0731\n",
      "Epoch [18/50], Loss: 0.0725\n",
      "Epoch [19/50], Loss: 0.0718\n",
      "Epoch [20/50], Loss: 0.0721\n",
      "Epoch [21/50], Loss: 0.0709\n",
      "Epoch [22/50], Loss: 0.0727\n",
      "Epoch [23/50], Loss: 0.0704\n",
      "Epoch [24/50], Loss: 0.0713\n",
      "Epoch [25/50], Loss: 0.0711\n",
      "Epoch [26/50], Loss: 0.0696\n",
      "Epoch [27/50], Loss: 0.0696\n",
      "Epoch [28/50], Loss: 0.0691\n",
      "Epoch [29/50], Loss: 0.0687\n",
      "Epoch [30/50], Loss: 0.0695\n",
      "Epoch [31/50], Loss: 0.0684\n",
      "Epoch [32/50], Loss: 0.0684\n",
      "Epoch [33/50], Loss: 0.0677\n",
      "Epoch [34/50], Loss: 0.0674\n",
      "Epoch [35/50], Loss: 0.0663\n",
      "Epoch [36/50], Loss: 0.0697\n",
      "Epoch [37/50], Loss: 0.0680\n",
      "Epoch [38/50], Loss: 0.0676\n",
      "Epoch [39/50], Loss: 0.0669\n",
      "Epoch [40/50], Loss: 0.0663\n",
      "Epoch [41/50], Loss: 0.0660\n",
      "Epoch [42/50], Loss: 0.0683\n",
      "Epoch [43/50], Loss: 0.0664\n",
      "Epoch [44/50], Loss: 0.0660\n",
      "Epoch [45/50], Loss: 0.0654\n",
      "Epoch [46/50], Loss: 0.0652\n",
      "Epoch [47/50], Loss: 0.0653\n",
      "Epoch [48/50], Loss: 0.0652\n",
      "Epoch [49/50], Loss: 0.0649\n",
      "Epoch [50/50], Loss: 0.0646\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 定义 UNet 模型（使用之前提供的 UNet 定义）\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(input_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "        self.center = self.conv_block(256, 512)\n",
    "        self.dec4 = self.conv_block(512 + 256, 256)\n",
    "        self.dec3 = self.conv_block(256 + 128, 128)\n",
    "        self.dec2 = self.conv_block(128 + 64, 64)\n",
    "        self.dec1 = self.conv_block(64 + 32, 32)\n",
    "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        enc1 = self.dropout(self.enc1(x))\n",
    "        enc2 = self.dropout(self.enc2(self.pool(enc1)))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        center = self.center(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([enc4, self.crop_and_concat(self.up(center), enc4)], 1))\n",
    "        dec3 = self.dec3(torch.cat([enc3, self.crop_and_concat(self.up(dec4), enc3)], 1))\n",
    "        dec2 = self.dec2(torch.cat([enc2, self.crop_and_concat(self.up(dec3), enc2)], 1))\n",
    "        dec1 = self.dec1(torch.cat([enc1, self.crop_and_concat(self.up(dec2), enc1)], 1))\n",
    "        final = self.final(dec1).squeeze()\n",
    "\n",
    "        return final\n",
    "    \n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        diffY = bypass.size()[2] - upsampled.size()[2]\n",
    "        diffX = bypass.size()[3] - upsampled.size()[3]\n",
    "\n",
    "        upsampled = torch.nn.functional.pad(upsampled, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        return upsampled\n",
    "\n",
    "# 定义数据集\n",
    "class RSDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images = self.read_multiband_images(images_dir)\n",
    "        self.labels = self.read_singleband_labels(labels_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "    def read_multiband_images(self, images_dir):\n",
    "        images = []\n",
    "        for image_file in os.listdir(images_dir):\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "            rsdl_data = gdal.Open(image_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open image file: {image_path}\")\n",
    "            images.append(np.stack([rsdl_data.GetRasterBand(i).ReadAsArray() for i in range(1, 4)], axis=0))\n",
    "        return images\n",
    "\n",
    "    def read_singleband_labels(self, labels_dir):\n",
    "        labels = []\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            rsdl_data = gdal.Open(label_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open label file: {label_path}\")\n",
    "            labels.append(rsdl_data.GetRasterBand(1).ReadAsArray())\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "# 定义数据增强和变换\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "def transform(image, label):\n",
    "    from PIL import Image\n",
    "    image = Image.fromarray(image)\n",
    "    label = Image.fromarray(label)\n",
    "\n",
    "    transform = get_transform()\n",
    "    image = transform(image)\n",
    "    label = transform(label)\n",
    "    \n",
    "    image = np.array(image)\n",
    "    label = np.array(label)\n",
    "    return image, label\n",
    "\n",
    "# 定义自定义 collate 函数\n",
    "def custom_collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        images = [image.clone().detach() for image in images]\n",
    "        labels = [label.clone().detach() for label in labels]\n",
    "        \n",
    "        max_width = max([img.shape[2] for img in images])\n",
    "        max_height = max([img.shape[1] for img in images])\n",
    "\n",
    "        padded_images = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for img, lbl in zip(images, labels):\n",
    "            pad_img = torch.zeros((img.shape[0], max_height, max_width))\n",
    "            pad_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "            pad_lbl = torch.zeros((max_height, max_width))\n",
    "            pad_lbl[:lbl.shape[0], :lbl.shape[1]] = lbl\n",
    "            padded_images.append(pad_img)\n",
    "            padded_labels.append(pad_lbl)\n",
    "        \n",
    "        return torch.stack(padded_images), torch.stack(padded_labels)\n",
    "\n",
    "# 定义训练参数和数据加载器\n",
    "images_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\train\\\\images/'\n",
    "labels_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\train\\\\masks'\n",
    "\n",
    "transform = lambda img, lbl: transform(img, lbl)\n",
    "dataset = RSDataset(images_dir, labels_dir, transform=transform)\n",
    "trainloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "model = UNet(input_channels=3, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device) / 255.0\n",
    "        outputs = model(images)\n",
    "        labels = labels.squeeze(0)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    torch.save(model.state_dict(), f'models_building_{epoch + 1}.pth')\n",
    "\n",
    "# 测试代码\n",
    "test_images_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\validation\\\\images/'\n",
    "test_labels_dir = 'E:\\\\数据集\\\\landslide4sense2022\\\\validation\\\\masks/'\n",
    "test_transform = get_transform()\n",
    "test_dataset = RSDataset(test_images_dir, test_labels_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.float().to(device) / 255.0\n",
    "            \n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            all_predictions.extend(outputs.flatten())\n",
    "            all_labels.extend(labels.flatten())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    average_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall curve (AP={average_precision:.2f})')\n",
    "    plt.show()\n",
    "print(precision)\n",
    "y_true, y_scores = evaluate_model(model, test_loader)\n",
    "plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.2978, Precision: 0.5529\n",
      "Epoch [2/500], Loss: 0.1679, Precision: 0.6795\n",
      "Epoch [3/500], Loss: 0.1435, Precision: 0.7188\n",
      "Epoch [4/500], Loss: 0.1323, Precision: 0.7427\n",
      "Epoch [5/500], Loss: 0.1282, Precision: 0.7533\n",
      "Epoch [6/500], Loss: 0.1194, Precision: 0.7733\n",
      "Epoch [7/500], Loss: 0.1169, Precision: 0.7774\n",
      "Epoch [8/500], Loss: 0.1192, Precision: 0.7809\n",
      "Epoch [9/500], Loss: 0.1123, Precision: 0.7859\n",
      "Epoch [10/500], Loss: 0.1117, Precision: 0.7899\n",
      "Epoch [11/500], Loss: 0.1091, Precision: 0.7909\n",
      "Epoch [12/500], Loss: 0.1105, Precision: 0.7956\n",
      "Epoch [13/500], Loss: 0.1099, Precision: 0.7979\n",
      "Epoch [14/500], Loss: 0.1023, Precision: 0.8114\n",
      "Epoch [15/500], Loss: 0.1033, Precision: 0.8080\n",
      "Epoch [16/500], Loss: 0.1016, Precision: 0.8093\n",
      "Epoch [17/500], Loss: 0.1000, Precision: 0.8172\n",
      "Epoch [18/500], Loss: 0.1030, Precision: 0.8101\n",
      "Epoch [19/500], Loss: 0.0989, Precision: 0.8207\n",
      "Epoch [20/500], Loss: 0.1007, Precision: 0.8149\n",
      "Epoch [21/500], Loss: 0.0954, Precision: 0.8220\n",
      "Epoch [22/500], Loss: 0.0985, Precision: 0.8162\n",
      "Epoch [23/500], Loss: 0.0959, Precision: 0.8274\n",
      "Epoch [24/500], Loss: 0.0923, Precision: 0.8275\n",
      "Epoch [25/500], Loss: 0.0922, Precision: 0.8333\n",
      "Epoch [26/500], Loss: 0.0921, Precision: 0.8341\n",
      "Epoch [27/500], Loss: 0.0919, Precision: 0.8284\n",
      "Epoch [28/500], Loss: 0.0904, Precision: 0.8353\n",
      "Epoch [29/500], Loss: 0.0894, Precision: 0.8374\n",
      "Epoch [30/500], Loss: 0.0878, Precision: 0.8376\n",
      "Epoch [31/500], Loss: 0.0868, Precision: 0.8402\n",
      "Epoch [32/500], Loss: 0.0861, Precision: 0.8395\n",
      "Epoch [33/500], Loss: 0.0844, Precision: 0.8430\n",
      "Epoch [34/500], Loss: 0.0837, Precision: 0.8430\n",
      "Epoch [35/500], Loss: 0.0835, Precision: 0.8420\n",
      "Epoch [36/500], Loss: 0.0833, Precision: 0.8448\n",
      "Epoch [37/500], Loss: 0.0847, Precision: 0.8474\n",
      "Epoch [38/500], Loss: 0.0821, Precision: 0.8470\n",
      "Epoch [39/500], Loss: 0.0794, Precision: 0.8494\n",
      "Epoch [40/500], Loss: 0.0814, Precision: 0.8486\n",
      "Epoch [41/500], Loss: 0.0797, Precision: 0.8512\n",
      "Epoch [42/500], Loss: 0.0806, Precision: 0.8514\n",
      "Epoch [43/500], Loss: 0.0798, Precision: 0.8499\n",
      "Epoch [44/500], Loss: 0.0788, Precision: 0.8541\n",
      "Epoch [45/500], Loss: 0.0766, Precision: 0.8546\n",
      "Epoch [46/500], Loss: 0.0788, Precision: 0.8548\n",
      "Epoch [47/500], Loss: 0.0791, Precision: 0.8533\n",
      "Epoch [48/500], Loss: 0.0792, Precision: 0.8512\n",
      "Epoch [49/500], Loss: 0.0738, Precision: 0.8600\n",
      "Epoch [50/500], Loss: 0.0745, Precision: 0.8567\n",
      "Epoch [51/500], Loss: 0.0774, Precision: 0.8533\n",
      "Epoch [52/500], Loss: 0.0740, Precision: 0.8605\n",
      "Epoch [53/500], Loss: 0.0730, Precision: 0.8628\n",
      "Epoch [54/500], Loss: 0.0731, Precision: 0.8627\n",
      "Epoch [55/500], Loss: 0.0714, Precision: 0.8616\n",
      "Epoch [56/500], Loss: 0.0732, Precision: 0.8613\n",
      "Epoch [57/500], Loss: 0.0728, Precision: 0.8631\n",
      "Epoch [58/500], Loss: 0.0716, Precision: 0.8654\n",
      "Epoch [59/500], Loss: 0.0733, Precision: 0.8628\n",
      "Epoch [60/500], Loss: 0.0693, Precision: 0.8702\n",
      "Epoch [61/500], Loss: 0.0693, Precision: 0.8665\n",
      "Epoch [62/500], Loss: 0.0690, Precision: 0.8689\n",
      "Epoch [63/500], Loss: 0.0694, Precision: 0.8636\n",
      "Epoch [64/500], Loss: 0.0689, Precision: 0.8694\n",
      "Epoch [65/500], Loss: 0.0670, Precision: 0.8719\n",
      "Epoch [66/500], Loss: 0.0679, Precision: 0.8692\n",
      "Epoch [67/500], Loss: 0.0686, Precision: 0.8699\n",
      "Epoch [68/500], Loss: 0.0656, Precision: 0.8699\n",
      "Epoch [69/500], Loss: 0.0680, Precision: 0.8699\n",
      "Epoch [70/500], Loss: 0.0671, Precision: 0.8689\n",
      "Epoch [71/500], Loss: 0.0654, Precision: 0.8744\n",
      "Epoch [72/500], Loss: 0.0658, Precision: 0.8724\n",
      "Epoch [73/500], Loss: 0.0640, Precision: 0.8744\n",
      "Epoch [74/500], Loss: 0.0668, Precision: 0.8700\n",
      "Epoch [75/500], Loss: 0.0647, Precision: 0.8728\n",
      "Epoch [76/500], Loss: 0.0618, Precision: 0.8783\n",
      "Epoch [77/500], Loss: 0.0634, Precision: 0.8776\n",
      "Epoch [78/500], Loss: 0.0630, Precision: 0.8757\n",
      "Epoch [79/500], Loss: 0.0621, Precision: 0.8790\n",
      "Epoch [80/500], Loss: 0.0630, Precision: 0.8763\n",
      "Epoch [81/500], Loss: 0.0622, Precision: 0.8796\n",
      "Epoch [82/500], Loss: 0.0610, Precision: 0.8759\n",
      "Epoch [83/500], Loss: 0.0616, Precision: 0.8768\n",
      "Epoch [84/500], Loss: 0.0604, Precision: 0.8806\n",
      "Epoch [85/500], Loss: 0.0584, Precision: 0.8818\n",
      "Epoch [86/500], Loss: 0.0594, Precision: 0.8813\n",
      "Epoch [87/500], Loss: 0.0593, Precision: 0.8813\n",
      "Epoch [88/500], Loss: 0.0569, Precision: 0.8857\n",
      "Epoch [89/500], Loss: 0.0569, Precision: 0.8853\n",
      "Epoch [90/500], Loss: 0.0563, Precision: 0.8849\n",
      "Epoch [91/500], Loss: 0.0591, Precision: 0.8822\n",
      "Epoch [92/500], Loss: 0.0576, Precision: 0.8850\n",
      "Epoch [93/500], Loss: 0.0567, Precision: 0.8839\n",
      "Epoch [94/500], Loss: 0.0558, Precision: 0.8877\n",
      "Epoch [95/500], Loss: 0.0549, Precision: 0.8880\n",
      "Epoch [96/500], Loss: 0.0545, Precision: 0.8887\n",
      "Epoch [97/500], Loss: 0.0532, Precision: 0.8906\n",
      "Epoch [98/500], Loss: 0.0544, Precision: 0.8887\n",
      "Epoch [99/500], Loss: 0.0554, Precision: 0.8877\n",
      "Epoch [100/500], Loss: 0.0510, Precision: 0.8914\n",
      "Epoch [101/500], Loss: 0.0529, Precision: 0.8906\n",
      "Epoch [102/500], Loss: 0.0512, Precision: 0.8927\n",
      "Epoch [103/500], Loss: 0.0520, Precision: 0.8919\n",
      "Epoch [104/500], Loss: 0.0523, Precision: 0.8908\n",
      "Epoch [105/500], Loss: 0.0531, Precision: 0.8903\n",
      "Epoch [106/500], Loss: 0.0518, Precision: 0.8911\n",
      "Epoch [107/500], Loss: 0.0524, Precision: 0.8922\n",
      "Epoch [108/500], Loss: 0.0500, Precision: 0.8941\n",
      "Epoch [109/500], Loss: 0.0516, Precision: 0.8914\n",
      "Epoch [110/500], Loss: 0.0512, Precision: 0.8946\n",
      "Epoch [111/500], Loss: 0.0501, Precision: 0.8931\n",
      "Epoch [112/500], Loss: 0.0497, Precision: 0.8951\n",
      "Epoch [113/500], Loss: 0.0477, Precision: 0.8978\n",
      "Epoch [114/500], Loss: 0.0495, Precision: 0.8951\n",
      "Epoch [115/500], Loss: 0.0464, Precision: 0.8990\n",
      "Epoch [116/500], Loss: 0.0489, Precision: 0.8991\n",
      "Epoch [117/500], Loss: 0.0471, Precision: 0.8988\n",
      "Epoch [118/500], Loss: 0.0481, Precision: 0.8973\n",
      "Epoch [119/500], Loss: 0.0489, Precision: 0.8968\n",
      "Epoch [120/500], Loss: 0.0472, Precision: 0.9000\n",
      "Epoch [121/500], Loss: 0.0468, Precision: 0.8992\n",
      "Epoch [122/500], Loss: 0.0515, Precision: 0.8900\n",
      "Epoch [123/500], Loss: 0.0452, Precision: 0.9027\n",
      "Epoch [124/500], Loss: 0.0456, Precision: 0.9003\n",
      "Epoch [125/500], Loss: 0.0431, Precision: 0.9047\n",
      "Epoch [126/500], Loss: 0.0440, Precision: 0.9014\n",
      "Epoch [127/500], Loss: 0.0464, Precision: 0.8991\n",
      "Epoch [128/500], Loss: 0.0428, Precision: 0.9047\n",
      "Epoch [129/500], Loss: 0.0450, Precision: 0.9018\n",
      "Epoch [130/500], Loss: 0.0440, Precision: 0.9014\n",
      "Epoch [131/500], Loss: 0.0439, Precision: 0.9028\n",
      "Epoch [132/500], Loss: 0.0455, Precision: 0.9009\n",
      "Epoch [133/500], Loss: 0.0432, Precision: 0.9055\n",
      "Epoch [134/500], Loss: 0.0453, Precision: 0.9017\n",
      "Epoch [135/500], Loss: 0.0443, Precision: 0.9044\n",
      "Epoch [136/500], Loss: 0.0423, Precision: 0.9077\n",
      "Epoch [137/500], Loss: 0.0431, Precision: 0.9059\n",
      "Epoch [138/500], Loss: 0.0429, Precision: 0.9023\n",
      "Epoch [139/500], Loss: 0.0413, Precision: 0.9065\n",
      "Epoch [140/500], Loss: 0.0419, Precision: 0.9065\n",
      "Epoch [141/500], Loss: 0.0415, Precision: 0.9069\n",
      "Epoch [142/500], Loss: 0.0405, Precision: 0.9099\n",
      "Epoch [143/500], Loss: 0.0440, Precision: 0.9032\n",
      "Epoch [144/500], Loss: 0.0451, Precision: 0.9006\n",
      "Epoch [145/500], Loss: 0.0403, Precision: 0.9092\n",
      "Epoch [146/500], Loss: 0.0393, Precision: 0.9105\n",
      "Epoch [147/500], Loss: 0.0409, Precision: 0.9086\n",
      "Epoch [148/500], Loss: 0.0393, Precision: 0.9115\n",
      "Epoch [149/500], Loss: 0.0419, Precision: 0.9079\n",
      "Epoch [150/500], Loss: 0.0398, Precision: 0.9076\n",
      "Epoch [151/500], Loss: 0.0429, Precision: 0.9065\n",
      "Epoch [152/500], Loss: 0.0416, Precision: 0.9087\n",
      "Epoch [153/500], Loss: 0.0407, Precision: 0.9078\n",
      "Epoch [154/500], Loss: 0.0404, Precision: 0.9089\n",
      "Epoch [155/500], Loss: 0.0410, Precision: 0.9073\n",
      "Epoch [156/500], Loss: 0.0394, Precision: 0.9092\n",
      "Epoch [157/500], Loss: 0.0462, Precision: 0.9003\n",
      "Epoch [158/500], Loss: 0.0412, Precision: 0.9073\n",
      "Epoch [159/500], Loss: 0.0359, Precision: 0.9168\n",
      "Epoch [160/500], Loss: 0.0378, Precision: 0.9126\n",
      "Epoch [161/500], Loss: 0.0384, Precision: 0.9101\n",
      "Epoch [162/500], Loss: 0.0402, Precision: 0.9071\n",
      "Epoch [163/500], Loss: 0.0419, Precision: 0.9072\n",
      "Epoch [164/500], Loss: 0.0382, Precision: 0.9139\n",
      "Epoch [165/500], Loss: 0.0377, Precision: 0.9130\n",
      "Epoch [166/500], Loss: 0.0372, Precision: 0.9139\n",
      "Epoch [167/500], Loss: 0.0419, Precision: 0.9078\n",
      "Epoch [168/500], Loss: 0.0372, Precision: 0.9130\n",
      "Epoch [169/500], Loss: 0.0364, Precision: 0.9158\n",
      "Epoch [170/500], Loss: 0.0358, Precision: 0.9164\n",
      "Epoch [171/500], Loss: 0.0378, Precision: 0.9123\n",
      "Epoch [172/500], Loss: 0.0427, Precision: 0.9053\n",
      "Epoch [173/500], Loss: 0.0360, Precision: 0.9162\n",
      "Epoch [174/500], Loss: 0.0368, Precision: 0.9142\n",
      "Epoch [175/500], Loss: 0.0370, Precision: 0.9130\n",
      "Epoch [176/500], Loss: 0.0424, Precision: 0.9070\n",
      "Epoch [177/500], Loss: 0.0368, Precision: 0.9144\n",
      "Epoch [178/500], Loss: 0.0347, Precision: 0.9176\n",
      "Epoch [179/500], Loss: 0.0378, Precision: 0.9127\n",
      "Epoch [180/500], Loss: 0.0361, Precision: 0.9160\n",
      "Epoch [181/500], Loss: 0.0384, Precision: 0.9117\n",
      "Epoch [182/500], Loss: 0.0394, Precision: 0.9133\n",
      "Epoch [183/500], Loss: 0.0356, Precision: 0.9176\n",
      "Epoch [184/500], Loss: 0.0354, Precision: 0.9163\n",
      "Epoch [185/500], Loss: 0.0340, Precision: 0.9193\n",
      "Epoch [186/500], Loss: 0.0353, Precision: 0.9161\n",
      "Epoch [187/500], Loss: 0.0366, Precision: 0.9132\n",
      "Epoch [188/500], Loss: 0.0374, Precision: 0.9135\n",
      "Epoch [189/500], Loss: 0.0344, Precision: 0.9185\n",
      "Epoch [190/500], Loss: 0.0368, Precision: 0.9157\n",
      "Epoch [191/500], Loss: 0.0389, Precision: 0.9134\n",
      "Epoch [192/500], Loss: 0.0345, Precision: 0.9191\n",
      "Epoch [193/500], Loss: 0.0340, Precision: 0.9197\n",
      "Epoch [194/500], Loss: 0.0360, Precision: 0.9153\n",
      "Epoch [195/500], Loss: 0.0368, Precision: 0.9134\n",
      "Epoch [196/500], Loss: 0.0347, Precision: 0.9188\n",
      "Epoch [197/500], Loss: 0.0345, Precision: 0.9180\n",
      "Epoch [198/500], Loss: 0.0365, Precision: 0.9162\n",
      "Epoch [199/500], Loss: 0.0345, Precision: 0.9195\n",
      "Epoch [200/500], Loss: 0.0359, Precision: 0.9162\n",
      "Epoch [201/500], Loss: 0.0330, Precision: 0.9200\n",
      "Epoch [202/500], Loss: 0.0449, Precision: 0.9015\n",
      "Epoch [203/500], Loss: 0.0333, Precision: 0.9202\n",
      "Epoch [204/500], Loss: 0.0313, Precision: 0.9227\n",
      "Epoch [205/500], Loss: 0.0344, Precision: 0.9178\n",
      "Epoch [206/500], Loss: 0.0351, Precision: 0.9179\n",
      "Epoch [207/500], Loss: 0.0341, Precision: 0.9162\n",
      "Epoch [208/500], Loss: 0.0382, Precision: 0.9126\n",
      "Epoch [209/500], Loss: 0.0327, Precision: 0.9200\n",
      "Epoch [210/500], Loss: 0.0330, Precision: 0.9217\n",
      "Epoch [211/500], Loss: 0.0326, Precision: 0.9212\n",
      "Epoch [212/500], Loss: 0.0364, Precision: 0.9160\n",
      "Epoch [213/500], Loss: 0.0318, Precision: 0.9217\n",
      "Epoch [214/500], Loss: 0.0334, Precision: 0.9208\n",
      "Epoch [215/500], Loss: 0.0358, Precision: 0.9171\n",
      "Epoch [216/500], Loss: 0.0322, Precision: 0.9212\n",
      "Epoch [217/500], Loss: 0.0353, Precision: 0.9171\n",
      "Epoch [218/500], Loss: 0.0382, Precision: 0.9134\n",
      "Epoch [219/500], Loss: 0.0321, Precision: 0.9236\n",
      "Epoch [220/500], Loss: 0.0316, Precision: 0.9225\n",
      "Epoch [221/500], Loss: 0.0338, Precision: 0.9204\n",
      "Epoch [222/500], Loss: 0.0333, Precision: 0.9215\n",
      "Epoch [223/500], Loss: 0.0342, Precision: 0.9189\n",
      "Epoch [224/500], Loss: 0.0353, Precision: 0.9175\n",
      "Epoch [225/500], Loss: 0.0314, Precision: 0.9231\n",
      "Epoch [226/500], Loss: 0.0315, Precision: 0.9238\n",
      "Epoch [227/500], Loss: 0.0334, Precision: 0.9216\n",
      "Epoch [228/500], Loss: 0.0311, Precision: 0.9238\n",
      "Epoch [229/500], Loss: 0.0410, Precision: 0.9115\n",
      "Epoch [230/500], Loss: 0.0318, Precision: 0.9227\n",
      "Epoch [231/500], Loss: 0.0306, Precision: 0.9258\n",
      "Epoch [232/500], Loss: 0.0373, Precision: 0.9151\n",
      "Epoch [233/500], Loss: 0.0309, Precision: 0.9242\n",
      "Epoch [234/500], Loss: 0.0308, Precision: 0.9256\n",
      "Epoch [235/500], Loss: 0.0306, Precision: 0.9250\n",
      "Epoch [236/500], Loss: 0.0306, Precision: 0.9254\n",
      "Epoch [237/500], Loss: 0.0351, Precision: 0.9188\n",
      "Epoch [238/500], Loss: 0.0400, Precision: 0.9124\n",
      "Epoch [239/500], Loss: 0.0305, Precision: 0.9255\n",
      "Epoch [240/500], Loss: 0.0326, Precision: 0.9199\n",
      "Epoch [241/500], Loss: 0.0324, Precision: 0.9218\n",
      "Epoch [242/500], Loss: 0.0317, Precision: 0.9235\n",
      "Epoch [243/500], Loss: 0.0344, Precision: 0.9184\n",
      "Epoch [244/500], Loss: 0.0329, Precision: 0.9220\n",
      "Epoch [245/500], Loss: 0.0324, Precision: 0.9248\n",
      "Epoch [246/500], Loss: 0.0303, Precision: 0.9247\n",
      "Epoch [247/500], Loss: 0.0304, Precision: 0.9262\n",
      "Epoch [248/500], Loss: 0.0368, Precision: 0.9159\n",
      "Epoch [249/500], Loss: 0.0360, Precision: 0.9174\n",
      "Epoch [250/500], Loss: 0.0298, Precision: 0.9261\n",
      "Epoch [251/500], Loss: 0.0314, Precision: 0.9243\n",
      "Epoch [252/500], Loss: 0.0303, Precision: 0.9265\n",
      "Epoch [253/500], Loss: 0.0302, Precision: 0.9248\n",
      "Epoch [254/500], Loss: 0.0349, Precision: 0.9212\n",
      "Epoch [255/500], Loss: 0.0316, Precision: 0.9243\n",
      "Epoch [256/500], Loss: 0.0300, Precision: 0.9263\n",
      "Epoch [257/500], Loss: 0.0327, Precision: 0.9232\n",
      "Epoch [258/500], Loss: 0.0319, Precision: 0.9227\n",
      "Epoch [259/500], Loss: 0.0298, Precision: 0.9259\n",
      "Epoch [260/500], Loss: 0.0304, Precision: 0.9265\n",
      "Epoch [261/500], Loss: 0.0358, Precision: 0.9186\n",
      "Epoch [262/500], Loss: 0.0311, Precision: 0.9266\n",
      "Epoch [263/500], Loss: 0.0280, Precision: 0.9289\n",
      "Epoch [264/500], Loss: 0.0291, Precision: 0.9277\n",
      "Epoch [265/500], Loss: 0.0336, Precision: 0.9198\n",
      "Epoch [266/500], Loss: 0.0371, Precision: 0.9161\n",
      "Epoch [267/500], Loss: 0.0299, Precision: 0.9259\n",
      "Epoch [268/500], Loss: 0.0285, Precision: 0.9275\n",
      "Epoch [269/500], Loss: 0.0288, Precision: 0.9277\n",
      "Epoch [270/500], Loss: 0.0343, Precision: 0.9201\n",
      "Epoch [271/500], Loss: 0.0322, Precision: 0.9247\n",
      "Epoch [272/500], Loss: 0.0286, Precision: 0.9289\n",
      "Epoch [273/500], Loss: 0.0296, Precision: 0.9267\n",
      "Epoch [274/500], Loss: 0.0315, Precision: 0.9234\n",
      "Epoch [275/500], Loss: 0.0301, Precision: 0.9255\n",
      "Epoch [276/500], Loss: 0.0326, Precision: 0.9224\n",
      "Epoch [277/500], Loss: 0.0296, Precision: 0.9258\n",
      "Epoch [278/500], Loss: 0.0290, Precision: 0.9285\n",
      "Epoch [279/500], Loss: 0.0448, Precision: 0.9070\n",
      "Epoch [280/500], Loss: 0.0344, Precision: 0.9183\n",
      "Epoch [281/500], Loss: 0.0285, Precision: 0.9280\n",
      "Epoch [282/500], Loss: 0.0293, Precision: 0.9275\n",
      "Epoch [283/500], Loss: 0.0350, Precision: 0.9187\n",
      "Epoch [284/500], Loss: 0.0304, Precision: 0.9265\n",
      "Epoch [285/500], Loss: 0.0285, Precision: 0.9296\n",
      "Epoch [286/500], Loss: 0.0293, Precision: 0.9281\n",
      "Epoch [287/500], Loss: 0.0319, Precision: 0.9240\n",
      "Epoch [288/500], Loss: 0.0311, Precision: 0.9239\n",
      "Epoch [289/500], Loss: 0.0288, Precision: 0.9286\n",
      "Epoch [290/500], Loss: 0.0324, Precision: 0.9214\n",
      "Epoch [291/500], Loss: 0.0304, Precision: 0.9259\n",
      "Epoch [292/500], Loss: 0.0327, Precision: 0.9223\n",
      "Epoch [293/500], Loss: 0.0284, Precision: 0.9300\n",
      "Epoch [294/500], Loss: 0.0278, Precision: 0.9314\n",
      "Epoch [295/500], Loss: 0.0308, Precision: 0.9261\n",
      "Epoch [296/500], Loss: 0.0319, Precision: 0.9258\n",
      "Epoch [297/500], Loss: 0.0296, Precision: 0.9261\n",
      "Epoch [298/500], Loss: 0.0291, Precision: 0.9296\n",
      "Epoch [299/500], Loss: 0.0295, Precision: 0.9262\n",
      "Epoch [300/500], Loss: 0.0393, Precision: 0.9144\n",
      "Epoch [301/500], Loss: 0.0287, Precision: 0.9284\n",
      "Epoch [302/500], Loss: 0.0260, Precision: 0.9329\n",
      "Epoch [303/500], Loss: 0.0313, Precision: 0.9243\n",
      "Epoch [304/500], Loss: 0.0286, Precision: 0.9288\n",
      "Epoch [305/500], Loss: 0.0278, Precision: 0.9309\n",
      "Epoch [306/500], Loss: 0.0296, Precision: 0.9279\n",
      "Epoch [307/500], Loss: 0.0294, Precision: 0.9282\n",
      "Epoch [308/500], Loss: 0.0316, Precision: 0.9234\n",
      "Epoch [309/500], Loss: 0.0283, Precision: 0.9307\n",
      "Epoch [310/500], Loss: 0.0274, Precision: 0.9316\n",
      "Epoch [311/500], Loss: 0.0427, Precision: 0.9142\n",
      "Epoch [312/500], Loss: 0.0382, Precision: 0.9169\n",
      "Epoch [313/500], Loss: 0.0279, Precision: 0.9298\n",
      "Epoch [314/500], Loss: 0.0267, Precision: 0.9319\n",
      "Epoch [315/500], Loss: 0.0294, Precision: 0.9288\n",
      "Epoch [316/500], Loss: 0.0273, Precision: 0.9326\n",
      "Epoch [317/500], Loss: 0.0281, Precision: 0.9299\n",
      "Epoch [318/500], Loss: 0.0301, Precision: 0.9272\n",
      "Epoch [319/500], Loss: 0.0322, Precision: 0.9238\n",
      "Epoch [320/500], Loss: 0.0295, Precision: 0.9284\n",
      "Epoch [321/500], Loss: 0.0329, Precision: 0.9216\n",
      "Epoch [322/500], Loss: 0.0301, Precision: 0.9262\n",
      "Epoch [323/500], Loss: 0.0290, Precision: 0.9282\n",
      "Epoch [324/500], Loss: 0.0283, Precision: 0.9295\n",
      "Epoch [325/500], Loss: 0.0300, Precision: 0.9285\n",
      "Epoch [326/500], Loss: 0.0285, Precision: 0.9287\n",
      "Epoch [327/500], Loss: 0.0275, Precision: 0.9308\n",
      "Epoch [328/500], Loss: 0.0296, Precision: 0.9291\n",
      "Epoch [329/500], Loss: 0.0281, Precision: 0.9304\n",
      "Epoch [330/500], Loss: 0.0301, Precision: 0.9258\n",
      "Epoch [331/500], Loss: 0.0432, Precision: 0.9105\n",
      "Epoch [332/500], Loss: 0.0278, Precision: 0.9308\n",
      "Epoch [333/500], Loss: 0.0267, Precision: 0.9322\n",
      "Epoch [334/500], Loss: 0.0287, Precision: 0.9295\n",
      "Epoch [335/500], Loss: 0.0295, Precision: 0.9299\n",
      "Epoch [336/500], Loss: 0.0286, Precision: 0.9310\n",
      "Epoch [337/500], Loss: 0.0293, Precision: 0.9295\n",
      "Epoch [338/500], Loss: 0.0304, Precision: 0.9267\n",
      "Epoch [339/500], Loss: 0.0269, Precision: 0.9332\n",
      "Epoch [340/500], Loss: 0.0280, Precision: 0.9316\n",
      "Epoch [341/500], Loss: 0.0282, Precision: 0.9297\n",
      "Epoch [342/500], Loss: 0.0441, Precision: 0.9059\n",
      "Epoch [343/500], Loss: 0.0292, Precision: 0.9294\n",
      "Epoch [344/500], Loss: 0.0274, Precision: 0.9312\n",
      "Epoch [345/500], Loss: 0.0269, Precision: 0.9331\n",
      "Epoch [346/500], Loss: 0.0297, Precision: 0.9287\n",
      "Epoch [347/500], Loss: 0.0302, Precision: 0.9272\n",
      "Epoch [348/500], Loss: 0.0267, Precision: 0.9319\n",
      "Epoch [349/500], Loss: 0.0333, Precision: 0.9254\n",
      "Epoch [350/500], Loss: 0.0318, Precision: 0.9242\n",
      "Epoch [351/500], Loss: 0.0319, Precision: 0.9228\n",
      "Epoch [352/500], Loss: 0.0261, Precision: 0.9344\n",
      "Epoch [353/500], Loss: 0.0262, Precision: 0.9337\n",
      "Epoch [354/500], Loss: 0.0283, Precision: 0.9305\n",
      "Epoch [355/500], Loss: 0.0278, Precision: 0.9318\n",
      "Epoch [356/500], Loss: 0.0286, Precision: 0.9318\n",
      "Epoch [357/500], Loss: 0.0295, Precision: 0.9289\n",
      "Epoch [358/500], Loss: 0.0279, Precision: 0.9313\n",
      "Epoch [359/500], Loss: 0.0271, Precision: 0.9319\n",
      "Epoch [360/500], Loss: 0.0380, Precision: 0.9186\n",
      "Epoch [361/500], Loss: 0.0349, Precision: 0.9189\n",
      "Epoch [362/500], Loss: 0.0262, Precision: 0.9343\n",
      "Epoch [363/500], Loss: 0.0259, Precision: 0.9349\n",
      "Epoch [364/500], Loss: 0.0265, Precision: 0.9341\n",
      "Epoch [365/500], Loss: 0.0294, Precision: 0.9292\n",
      "Epoch [366/500], Loss: 0.0286, Precision: 0.9294\n",
      "Epoch [367/500], Loss: 0.0281, Precision: 0.9313\n",
      "Epoch [368/500], Loss: 0.0280, Precision: 0.9309\n",
      "Epoch [369/500], Loss: 0.0312, Precision: 0.9257\n",
      "Epoch [370/500], Loss: 0.0282, Precision: 0.9310\n",
      "Epoch [371/500], Loss: 0.0264, Precision: 0.9341\n",
      "Epoch [372/500], Loss: 0.0265, Precision: 0.9328\n",
      "Epoch [373/500], Loss: 0.0313, Precision: 0.9264\n",
      "Epoch [374/500], Loss: 0.0280, Precision: 0.9317\n",
      "Epoch [375/500], Loss: 0.0257, Precision: 0.9350\n",
      "Epoch [376/500], Loss: 0.0266, Precision: 0.9325\n",
      "Epoch [377/500], Loss: 0.0273, Precision: 0.9330\n",
      "Epoch [378/500], Loss: 0.0271, Precision: 0.9331\n",
      "Epoch [379/500], Loss: 0.0322, Precision: 0.9243\n",
      "Epoch [380/500], Loss: 0.0275, Precision: 0.9315\n",
      "Epoch [381/500], Loss: 0.0286, Precision: 0.9303\n",
      "Epoch [382/500], Loss: 0.0360, Precision: 0.9176\n",
      "Epoch [383/500], Loss: 0.0265, Precision: 0.9338\n",
      "Epoch [384/500], Loss: 0.0256, Precision: 0.9356\n",
      "Epoch [385/500], Loss: 0.0253, Precision: 0.9358\n",
      "Epoch [386/500], Loss: 0.0338, Precision: 0.9221\n",
      "Epoch [387/500], Loss: 0.0279, Precision: 0.9307\n",
      "Epoch [388/500], Loss: 0.0275, Precision: 0.9316\n",
      "Epoch [389/500], Loss: 0.0267, Precision: 0.9342\n",
      "Epoch [390/500], Loss: 0.0275, Precision: 0.9321\n",
      "Epoch [391/500], Loss: 0.0276, Precision: 0.9316\n",
      "Epoch [392/500], Loss: 0.0288, Precision: 0.9301\n",
      "Epoch [393/500], Loss: 0.0415, Precision: 0.9116\n",
      "Epoch [394/500], Loss: 0.0264, Precision: 0.9320\n",
      "Epoch [395/500], Loss: 0.0250, Precision: 0.9362\n",
      "Epoch [396/500], Loss: 0.0252, Precision: 0.9367\n",
      "Epoch [397/500], Loss: 0.0259, Precision: 0.9351\n",
      "Epoch [398/500], Loss: 0.0362, Precision: 0.9162\n",
      "Epoch [399/500], Loss: 0.0263, Precision: 0.9332\n",
      "Epoch [400/500], Loss: 0.0261, Precision: 0.9344\n",
      "Epoch [401/500], Loss: 0.0258, Precision: 0.9336\n",
      "Epoch [402/500], Loss: 0.0277, Precision: 0.9325\n",
      "Epoch [403/500], Loss: 0.0277, Precision: 0.9321\n",
      "Epoch [404/500], Loss: 0.0313, Precision: 0.9250\n",
      "Epoch [405/500], Loss: 0.0268, Precision: 0.9337\n",
      "Epoch [406/500], Loss: 0.0267, Precision: 0.9331\n",
      "Epoch [407/500], Loss: 0.0253, Precision: 0.9353\n",
      "Epoch [408/500], Loss: 0.0261, Precision: 0.9340\n",
      "Epoch [409/500], Loss: 0.0380, Precision: 0.9146\n",
      "Epoch [410/500], Loss: 0.0276, Precision: 0.9315\n",
      "Epoch [411/500], Loss: 0.0259, Precision: 0.9346\n",
      "Epoch [412/500], Loss: 0.0258, Precision: 0.9353\n",
      "Epoch [413/500], Loss: 0.0256, Precision: 0.9361\n",
      "Epoch [414/500], Loss: 0.0276, Precision: 0.9313\n",
      "Epoch [415/500], Loss: 0.0308, Precision: 0.9263\n",
      "Epoch [416/500], Loss: 0.0265, Precision: 0.9341\n",
      "Epoch [417/500], Loss: 0.0269, Precision: 0.9315\n",
      "Epoch [418/500], Loss: 0.0298, Precision: 0.9301\n",
      "Epoch [419/500], Loss: 0.0387, Precision: 0.9148\n",
      "Epoch [420/500], Loss: 0.0254, Precision: 0.9353\n",
      "Epoch [421/500], Loss: 0.0249, Precision: 0.9371\n",
      "Epoch [422/500], Loss: 0.0242, Precision: 0.9386\n",
      "Epoch [423/500], Loss: 0.0254, Precision: 0.9355\n",
      "Epoch [424/500], Loss: 0.0294, Precision: 0.9292\n",
      "Epoch [425/500], Loss: 0.0299, Precision: 0.9283\n",
      "Epoch [426/500], Loss: 0.0256, Precision: 0.9350\n",
      "Epoch [427/500], Loss: 0.0339, Precision: 0.9248\n",
      "Epoch [428/500], Loss: 0.0342, Precision: 0.9226\n",
      "Epoch [429/500], Loss: 0.0256, Precision: 0.9351\n",
      "Epoch [430/500], Loss: 0.0240, Precision: 0.9379\n",
      "Epoch [431/500], Loss: 0.0275, Precision: 0.9311\n",
      "Epoch [432/500], Loss: 0.0258, Precision: 0.9350\n",
      "Epoch [433/500], Loss: 0.0253, Precision: 0.9351\n",
      "Epoch [434/500], Loss: 0.0274, Precision: 0.9330\n",
      "Epoch [435/500], Loss: 0.0291, Precision: 0.9299\n",
      "Epoch [436/500], Loss: 0.0297, Precision: 0.9283\n",
      "Epoch [437/500], Loss: 0.0256, Precision: 0.9354\n",
      "Epoch [438/500], Loss: 0.0249, Precision: 0.9367\n",
      "Epoch [439/500], Loss: 0.0270, Precision: 0.9314\n",
      "Epoch [440/500], Loss: 0.0272, Precision: 0.9330\n",
      "Epoch [441/500], Loss: 0.0270, Precision: 0.9337\n",
      "Epoch [442/500], Loss: 0.0314, Precision: 0.9275\n",
      "Epoch [443/500], Loss: 0.0264, Precision: 0.9329\n",
      "Epoch [444/500], Loss: 0.0318, Precision: 0.9237\n",
      "Epoch [445/500], Loss: 0.0247, Precision: 0.9380\n",
      "Epoch [446/500], Loss: 0.0250, Precision: 0.9362\n",
      "Epoch [447/500], Loss: 0.0273, Precision: 0.9326\n",
      "Epoch [448/500], Loss: 0.0299, Precision: 0.9295\n",
      "Epoch [449/500], Loss: 0.0246, Precision: 0.9376\n",
      "Epoch [450/500], Loss: 0.0280, Precision: 0.9309\n",
      "Epoch [451/500], Loss: 0.0272, Precision: 0.9310\n",
      "Epoch [452/500], Loss: 0.0329, Precision: 0.9267\n",
      "Epoch [453/500], Loss: 0.0357, Precision: 0.9201\n",
      "Epoch [454/500], Loss: 0.0256, Precision: 0.9351\n",
      "Epoch [455/500], Loss: 0.0236, Precision: 0.9385\n",
      "Epoch [456/500], Loss: 0.0265, Precision: 0.9348\n",
      "Epoch [457/500], Loss: 0.0286, Precision: 0.9311\n",
      "Epoch [458/500], Loss: 0.0285, Precision: 0.9300\n",
      "Epoch [459/500], Loss: 0.0267, Precision: 0.9349\n",
      "Epoch [460/500], Loss: 0.0259, Precision: 0.9345\n",
      "Epoch [461/500], Loss: 0.0246, Precision: 0.9368\n",
      "Epoch [462/500], Loss: 0.0263, Precision: 0.9348\n",
      "Epoch [463/500], Loss: 0.0265, Precision: 0.9338\n",
      "Epoch [464/500], Loss: 0.0297, Precision: 0.9295\n",
      "Epoch [465/500], Loss: 0.0274, Precision: 0.9335\n",
      "Epoch [466/500], Loss: 0.0256, Precision: 0.9334\n",
      "Epoch [467/500], Loss: 0.0246, Precision: 0.9377\n",
      "Epoch [468/500], Loss: 0.0284, Precision: 0.9327\n",
      "Epoch [469/500], Loss: 0.0341, Precision: 0.9219\n",
      "Epoch [470/500], Loss: 0.0249, Precision: 0.9368\n",
      "Epoch [471/500], Loss: 0.0245, Precision: 0.9381\n",
      "Epoch [472/500], Loss: 0.0309, Precision: 0.9267\n",
      "Epoch [473/500], Loss: 0.0273, Precision: 0.9339\n",
      "Epoch [474/500], Loss: 0.0239, Precision: 0.9381\n",
      "Epoch [475/500], Loss: 0.0263, Precision: 0.9354\n",
      "Epoch [476/500], Loss: 0.0268, Precision: 0.9328\n",
      "Epoch [477/500], Loss: 0.0256, Precision: 0.9352\n",
      "Epoch [478/500], Loss: 0.0252, Precision: 0.9364\n",
      "Epoch [479/500], Loss: 0.0307, Precision: 0.9287\n",
      "Epoch [480/500], Loss: 0.0368, Precision: 0.9160\n",
      "Epoch [481/500], Loss: 0.0253, Precision: 0.9361\n",
      "Epoch [482/500], Loss: 0.0234, Precision: 0.9387\n",
      "Epoch [483/500], Loss: 0.0255, Precision: 0.9355\n",
      "Epoch [484/500], Loss: 0.0256, Precision: 0.9342\n",
      "Epoch [485/500], Loss: 0.0275, Precision: 0.9330\n",
      "Epoch [486/500], Loss: 0.0371, Precision: 0.9185\n",
      "Epoch [487/500], Loss: 0.0252, Precision: 0.9361\n",
      "Epoch [488/500], Loss: 0.0240, Precision: 0.9385\n",
      "Epoch [489/500], Loss: 0.0239, Precision: 0.9385\n",
      "Epoch [490/500], Loss: 0.0284, Precision: 0.9315\n",
      "Epoch [491/500], Loss: 0.0265, Precision: 0.9327\n",
      "Epoch [492/500], Loss: 0.0249, Precision: 0.9370\n",
      "Epoch [493/500], Loss: 0.0281, Precision: 0.9320\n",
      "Epoch [494/500], Loss: 0.0265, Precision: 0.9341\n",
      "Epoch [495/500], Loss: 0.0263, Precision: 0.9341\n",
      "Epoch [496/500], Loss: 0.0251, Precision: 0.9367\n",
      "Epoch [497/500], Loss: 0.0258, Precision: 0.9350\n",
      "Epoch [498/500], Loss: 0.0533, Precision: 0.8932\n",
      "Epoch [499/500], Loss: 0.0292, Precision: 0.9285\n",
      "Epoch [500/500], Loss: 0.0244, Precision: 0.9366\n",
      "0.9265402843601895\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 定义 UNet 模型（使用之前提供的 UNet 定义）\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(input_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "        self.center = self.conv_block(256, 512)\n",
    "        self.dec4 = self.conv_block(512 + 256, 256)\n",
    "        self.dec3 = self.conv_block(256 + 128, 128)\n",
    "        self.dec2 = self.conv_block(128 + 64, 64)\n",
    "        self.dec1 = self.conv_block(64 + 32, 32)\n",
    "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        enc1 = self.dropout(self.enc1(x))\n",
    "        enc2 = self.dropout(self.enc2(self.pool(enc1)))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        center = self.center(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([enc4, self.crop_and_concat(self.up(center), enc4)], 1))\n",
    "        dec3 = self.dec3(torch.cat([enc3, self.crop_and_concat(self.up(dec4), enc3)], 1))\n",
    "        dec2 = self.dec2(torch.cat([enc2, self.crop_and_concat(self.up(dec3), enc2)], 1))\n",
    "        dec1 = self.dec1(torch.cat([enc1, self.crop_and_concat(self.up(dec2), enc1)], 1))\n",
    "        final = self.final(dec1).squeeze()\n",
    "\n",
    "        return final\n",
    "    \n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        diffY = bypass.size()[2] - upsampled.size()[2]\n",
    "        diffX = bypass.size()[3] - upsampled.size()[3]\n",
    "        upsampled = torch.nn.functional.pad(upsampled, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        return upsampled\n",
    "\n",
    "# 定义数据集\n",
    "class RSDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images = self.read_multiband_images(images_dir)\n",
    "        self.labels = self.read_singleband_labels(labels_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "    def read_multiband_images(self, images_dir):\n",
    "        images = []\n",
    "        for image_file in os.listdir(images_dir):\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "            rsdl_data = gdal.Open(image_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open image file: {image_path}\")\n",
    "            images.append(np.stack([rsdl_data.GetRasterBand(i).ReadAsArray() for i in range(1, 4)], axis=0))\n",
    "        return images\n",
    "\n",
    "    def read_singleband_labels(self, labels_dir):\n",
    "        labels = []\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            rsdl_data = gdal.Open(label_path)\n",
    "            if rsdl_data is None:\n",
    "                raise FileNotFoundError(f\"Unable to open label file: {label_path}\")\n",
    "            labels.append(rsdl_data.GetRasterBand(1).ReadAsArray())\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "# 定义数据增强和变换\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "def transform(image, label):\n",
    "    from PIL import Image\n",
    "    image = Image.fromarray(image)\n",
    "    label = Image.fromarray(label)\n",
    "\n",
    "    transform = get_transform()\n",
    "    image = transform(image)\n",
    "    label = transform(label)\n",
    "    \n",
    "    image = np.array(image)\n",
    "    label = np.array(label)\n",
    "    return image, label\n",
    "\n",
    "# 定义自定义 collate 函数\n",
    "def custom_collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        images = [image.clone().detach() for image in images]\n",
    "        labels = [label.clone().detach() for label in labels]\n",
    "        \n",
    "        max_width = max([img.shape[2] for img in images])\n",
    "        max_height = max([img.shape[1] for img in images])\n",
    "\n",
    "        padded_images = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for img, lbl in zip(images, labels):\n",
    "            pad_img = torch.zeros((img.shape[0], max_height, max_width))\n",
    "            pad_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "            pad_lbl = torch.zeros((max_height, max_width))\n",
    "            pad_lbl[:lbl.shape[0], :lbl.shape[1]] = lbl\n",
    "            padded_images.append(pad_img)\n",
    "            padded_labels.append(pad_lbl)\n",
    "        \n",
    "        return torch.stack(padded_images), torch.stack(padded_labels)\n",
    "\n",
    "# 定义训练参数和数据加载器\n",
    "images_dir = 'E:\\\\数据集\\\\山体滑坡数据集\\\\landslide\\\\image3'\n",
    "labels_dir = 'E:\\\\数据集\\\\山体滑坡数据集\\\\landslide\\\\mask3'\n",
    "\n",
    "transform = lambda img, lbl: transform(img, lbl)\n",
    "dataset = RSDataset(images_dir, labels_dir, transform=transform)\n",
    "trainloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "model = UNet(input_channels=3, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "def calculate_precision(outputs, labels):\n",
    "    # 使用detach()去除梯度追踪，然后转换为numpy\n",
    "    outputs = torch.sigmoid(outputs).detach().cpu().numpy().flatten() > 0.5\n",
    "    labels = labels.detach().cpu().numpy().flatten()\n",
    "    \n",
    "    # 计算精度，添加 zero_division 参数以处理除以零的情况\n",
    "    precision = precision_score(labels, outputs, zero_division=1)\n",
    "    return precision\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_precision = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device) / 255.0\n",
    "        outputs = model(images)\n",
    "        labels = labels.squeeze(0)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        precision = calculate_precision(outputs, labels)\n",
    "        running_precision += precision\n",
    "\n",
    "    epoch_precision = running_precision / len(trainloader)    \n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Precision: {epoch_precision:.4f}')\n",
    "    torch.save(model.state_dict(), f'models_building_{epoch + 1}.pth')\n",
    "\n",
    "# 测试代码\n",
    "test_images_dir = 'E:\\\\数据集\\\\山体滑坡数据集\\\\landslide\\\\image1'\n",
    "test_labels_dir = 'E:\\\\数据集\\\\山体滑坡数据集\\\\landslide\\\\mask1'\n",
    "test_transform = get_transform()\n",
    "test_dataset = RSDataset(test_images_dir, test_labels_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.float().to(device) / 255.0\n",
    "            \n",
    "            outputs = model(images).squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            all_predictions.extend(outputs.flatten())\n",
    "            all_labels.extend(labels.flatten())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    average_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall curve (AP={average_precision:.2f})')\n",
    "    plt.show()\n",
    "print(precision)\n",
    "y_true, y_scores = evaluate_model(model, test_loader)\n",
    "plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黑白图像已保存到 black_and_white_image.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def convert_to_black_and_white(input_image_path, output_image_path):\n",
    "    # 打开原始图像\n",
    "    image = Image.open(input_image_path)\n",
    "    \n",
    "    # 转换为黑白图像\n",
    "    bw_image = image.convert('L')\n",
    "    \n",
    "    # 保存黑白图像\n",
    "    bw_image.save(output_image_path)\n",
    "    \n",
    "    print(f\"黑白图像已保存到 {output_image_path}\")\n",
    "\n",
    "# 使用示例\n",
    "input_image_path = 'E:\\\\repository\\\\unet.png'\n",
    "output_image_path = 'black_and_white_image.jpg'\n",
    "convert_to_black_and_white(input_image_path, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m l1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     31\u001b[0m l2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mSolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddTwoNumbers\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml2\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mSolution.addTwoNumbers\u001b[1;34m(self, l1, l2)\u001b[0m\n\u001b[0;32m     11\u001b[0m carry \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m l1 \u001b[38;5;129;01mor\u001b[39;00m l2 \u001b[38;5;129;01mor\u001b[39;00m carry:\n\u001b[1;32m---> 14\u001b[0m     val1 \u001b[38;5;241m=\u001b[39m \u001b[43ml1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m l1 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     15\u001b[0m     val2 \u001b[38;5;241m=\u001b[39m l2\u001b[38;5;241m.\u001b[39mval \u001b[38;5;28;01mif\u001b[39;00m l2 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m     total \u001b[38;5;241m=\u001b[39m val1 \u001b[38;5;241m+\u001b[39m val2 \u001b[38;5;241m+\u001b[39m carry\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'val'"
     ]
    }
   ],
   "source": [
    "#两数相加\n",
    "class ListNode:\n",
    "    def __init__(self, val=0, next=None):\n",
    "        self.val = val\n",
    "        self.next = next\n",
    "\n",
    "class Solution:\n",
    "    def addTwoNumbers(self, l1, l2):\n",
    "        dummy_head = ListNode(0)\n",
    "        current = dummy_head\n",
    "        carry = 0\n",
    "\n",
    "        while l1 or l2 or carry:\n",
    "            val1 = l1.val if l1 else 0\n",
    "            val2 = l2.val if l2 else 0\n",
    "\n",
    "            total = val1 + val2 + carry\n",
    "            carry = total // 10\n",
    "            current.next = ListNode(total % 10)\n",
    "            current = current.next\n",
    "\n",
    "            if l1:\n",
    "                l1 = l1.next\n",
    "            if l2:\n",
    "                l2 = l2.next\n",
    "\n",
    "        return dummy_head.next\n",
    "\n",
    "\n",
    "Solution=Solution()\n",
    "l1 = [2,4,3]\n",
    "l2 = [5,6,4]\n",
    "print(Solution.addTwoNumbers(l1,l2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\龙儿璨\\AppData\\Local\\Temp\\ipykernel_34600\\4202562417.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Compressive Transformer\n",
    "summary: >\n",
    "  Documented implementation with explanations of a\n",
    "  Compressive Transformer model.\n",
    "---\n",
    "\n",
    "# Compressive Transformer\n",
    "\n",
    "This is an implementation of\n",
    "[Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)\n",
    "in [PyTorch](https://pytorch.org).\n",
    "\n",
    "This is an extension of [Transformer XL](../xl/index.html) where past memories\n",
    "are compressed to give a longer attention range.\n",
    "That is, the furthest $n_{cm} c$ memories are compressed into\n",
    "$n_{cm}$ memories, where $c$ is the compression rate.\n",
    "\n",
    "## Compression operation\n",
    "\n",
    "The compression operation is defined as\n",
    "$f_c: \\mathbb{R}^{nc \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$.\n",
    "The paper introduces multiple choices for $f_c$ and we have only implemented\n",
    "1D convolution which seems to give the best results.\n",
    "Each layer has a separate compression operation $f_c^{(i)}$ where\n",
    "$i$ is the layer number.\n",
    "\n",
    "## Training compression operation\n",
    "\n",
    "Since training compression with BPTT requires maintaining\n",
    "a very large computational graph (many time steps), the paper proposes\n",
    "an *auto-encoding loss* and an *attention reconstruction loss*.\n",
    "The auto-encoding loss decodes the original memories from the compressed memories\n",
    "and calculates the loss.\n",
    "Attention reconstruction loss computes the multi-headed attention results\n",
    "on the compressed memory and on uncompressed memory and gets a mean squared error\n",
    "between them.\n",
    "We have implemented the latter here since it gives better results.\n",
    "\n",
    "This implementation uses pre-layer normalization\n",
    "while the paper uses post-layer normalization.\n",
    "Pre-layer norm does the layer norm before [FFN](../feedforward.html) and\n",
    "self-attention, and the pass-through in the residual connection is not normalized.\n",
    "This is supposed to be more stable in standard transformer setups.\n",
    "\n",
    "Here are [the training code](experiment.html) and a notebook for training a compressive transformer\n",
    "model on the Tiny Shakespeare dataset.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb)\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from labml_helpers.module import Module, TypedModuleList\n",
    "from labml_nn.transformers.feed_forward import FeedForward\n",
    "from labml_nn.transformers.mha import PrepareForMultiHeadAttention\n",
    "from labml_nn.transformers.xl.relative_mha import RelativeMultiHeadAttention\n",
    "from labml_nn.utils import clone_module_list\n",
    "\n",
    "\n",
    "class Conv1dCompression(Module):\n",
    "    \"\"\"\n",
    "    ## 1D Convolution Compression $f_c$\n",
    "\n",
    "    This is a simple wrapper around\n",
    "    [`nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "    with some tensor dimension permutations.\n",
    "    \"\"\"\n",
    "    def __init__(self, compression_rate: int, d_model: int):\n",
    "        \"\"\"\n",
    "        * `compression_rate` $c$\n",
    "        * `d_model` is the embedding size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=compression_rate, stride=compression_rate)\n",
    "\n",
    "    def forward(self, mem: torch.Tensor):\n",
    "        \"\"\"\n",
    "        `mem` has shape `[seq_len, batch, d_model]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Permute the dimensions of `mem` so that we can run it through the convolution layer.\n",
    "        # The convolution layer accepts in the form `[batch, features, sequence]`\n",
    "        mem = mem.permute(1, 2, 0)\n",
    "        # Get compressed memory by running it through the convolution layer\n",
    "        c_mem = self.conv(mem)\n",
    "        # Permute back to form `[seq_len, batch, d_model]`\n",
    "        return c_mem.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "class CompressiveTransformerLayer(Module):\n",
    "    \"\"\"\n",
    "    ## Compressive Transformer Layer\n",
    "\n",
    "    This is the implementation of a single compressive transformer layer\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 d_model: int,\n",
    "                 self_attn: RelativeMultiHeadAttention,\n",
    "                 feed_forward: FeedForward,\n",
    "                 dropout_prob: float,\n",
    "                 compress: Conv1dCompression):\n",
    "        \"\"\"\n",
    "        * `d_model` is the token embedding size\n",
    "        * `self_attn` is the [self attention module](../xl/relative_mha.html)\n",
    "        * `feed_forward` is the [feed forward module](../feed_forward.html)\n",
    "        * `dropout_prob` is the probability of dropping out after self attention and FFN\n",
    "        * `compress` is the compression function $f_c$\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.compress = compress\n",
    "        self.size = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.norm_self_attn = nn.LayerNorm([d_model])\n",
    "        self.norm_ff = nn.LayerNorm([d_model])\n",
    "\n",
    "    def concat_memory(self, z: torch.Tensor, mem: Optional[torch.Tensor], c_mem: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Concatenate the normalized token embeddings with memory and compressed memory.\n",
    "\n",
    "        * `z` is layer normalized token embeddings.\n",
    "        * `mem` and `c_mem` are memory and compressed memory (not normalized).\n",
    "        \"\"\"\n",
    "\n",
    "        # If there is no memory just return the token embeddings\n",
    "        if mem is None:\n",
    "            return z\n",
    "\n",
    "        # If there are compressed memory concatenate that with memory\n",
    "        if c_mem is not None:\n",
    "            mem = torch.cat((c_mem, mem), dim=0)\n",
    "\n",
    "        # Run the memory through the normalization layer\n",
    "        mem = self.norm_self_attn(mem)\n",
    "        # Concatenate normalized memory and normalized token embeddings\n",
    "        return torch.cat((mem, z), dim=0)\n",
    "\n",
    "    def forward(self, *,\n",
    "                x: torch.Tensor,\n",
    "                mem: Optional[torch.Tensor],\n",
    "                c_mem: Optional[torch.Tensor],\n",
    "                mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is a tensor of token level feature vectors of shape `[seq_len, batch_size, d_model]`\n",
    "        * `mem` is a tensor of the past token level feature vectors (memory) of shape `[mem_len, batch_size, d_model]`\n",
    "        * `c_mem` is a tensor of the compressed memory `[c_mem_len, batch_size, d_model]`\n",
    "        * `mask` is a matrix of shape `[seq_len, c_mem_len + mem_len + seq_len, batch_size]` or `[seq_len, c_mem_len + mem_len + seq_len, 1]`.\n",
    "        `mask[i, j]` is  true if token at `i` can see token at `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Normalize the vectors before doing self attention\n",
    "        z = self.norm_self_attn(x)\n",
    "        # Normalize and concatenate memory and compressed memory\n",
    "        m_z = self.concat_memory(z, mem, c_mem)\n",
    "        # Attention\n",
    "        self_attn = self.self_attn(query=z, key=m_z, value=m_z, mask=mask)\n",
    "        # Add the attention results\n",
    "        x = x + self.dropout(self_attn)\n",
    "\n",
    "        # Normalize for feed-forward\n",
    "        z = self.norm_ff(x)\n",
    "        # Pass through the feed-forward network\n",
    "        ff = self.feed_forward(z)\n",
    "        # Add the feed-forward results back\n",
    "        x = x + self.dropout(ff)\n",
    "\n",
    "        #\n",
    "        return x\n",
    "\n",
    "\n",
    "class CompressiveTransformer(Module):\n",
    "    \"\"\"\n",
    "    ## Compressive Transformer Model\n",
    "\n",
    "    This consists of multiple compressive transformer layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer: CompressiveTransformerLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        # Make copies of the transformer layer\n",
    "        self.layers = clone_module_list(layer, n_layers)\n",
    "        # Final normalization layer\n",
    "        self.norm = nn.LayerNorm([layer.size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: List[torch.Tensor], c_mem: List[torch.Tensor], mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is a tensor of the token embeddings vectors of shape `[seq_len, batch_size, d_model]`\n",
    "        * `mem` is a list of tensors of the past token level feature vectors of shape\n",
    "         `[mem_len, batch_size, d_model]` for each layer\n",
    "        * `c_mem` is a list of tensors of the compressed memory\n",
    "         `[c_mem_len, batch_size, d_model]` for each layer\n",
    "        * `mask` is the masking matrix\n",
    "        \"\"\"\n",
    "        # List to store token level feature vectors,\n",
    "        # which will become the memories for the next sequential batch.\n",
    "        new_mem = []\n",
    "        # Run through each transformer layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Add to the list of feature vectors\n",
    "            new_mem.append(x.detach())\n",
    "            # Memory\n",
    "            m = mem[i] if mem else None\n",
    "            # Compressed Memory\n",
    "            cm = c_mem[i] if c_mem else None\n",
    "            # Run through the transformer XL layer\n",
    "            x = layer(x=x, mem=m, c_mem=cm, mask=mask)\n",
    "        # Finally, normalize the vectors\n",
    "        return self.norm(x), new_mem\n",
    "\n",
    "\n",
    "class AttentionReconstructionLoss:\n",
    "    \"\"\"\n",
    "    ## Attention Reconstruction Loss\n",
    "\n",
    "    Attention reconstruction loss recreates the self-attention output with\n",
    "    uncompressed memory and with compressed memory and calculates the mean squared error\n",
    "    between the two. It does this without positional encoding.\n",
    "\n",
    "    When calculating and training the compression function $f_c$ with attention\n",
    "    reconstruction loss, all parameters but $f_c$ are frozen.\n",
    "    This includes key/value projections and bias/scaling after normalization.\n",
    "\n",
    "    Since this loss can be computed independently of the cross-entropy-loss of the model\n",
    "    you can have a separate optimizer that only updates $f_c$.\n",
    "    However, we use the same optimizer to update $f_c$ so when calculating\n",
    "    attention reconstruction loss, we detach all other parameters except $f_c$\n",
    "    from the gradient computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: TypedModuleList[CompressiveTransformerLayer]):\n",
    "        \"\"\"\n",
    "        `layers` is the list of Compressive Transformer layers\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def prepare_for_attn(self, pmha: PrepareForMultiHeadAttention, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        This is a reimplementation of ['PrepareForMultiHeadAttention'](../mha.html#PrepareMHA)\n",
    "        where the projections are done with the parameters detached from gradient computation.\n",
    "\n",
    "        * `pmha` is the ['PrepareForMultiHeadAttention'](../mha.html#PrepareMHA) module\n",
    "        * `x` is tensor with the token embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Shape of the input except embedding dimension; `[seq_len, batch_size]`.\n",
    "        head_shape = x.shape[:-1]\n",
    "\n",
    "        # Detach projection weights and bias\n",
    "        weight = pmha.linear.weight.detach()\n",
    "        bias = pmha.linear.bias.detach() if pmha.linear.bias is not None else None\n",
    "        # Linear transform\n",
    "        x = F.linear(x, weight, bias)\n",
    "\n",
    "        # Split last dimension into heads\n",
    "        x = x.view(*head_shape, pmha.heads, pmha.d_k)\n",
    "\n",
    "        # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, d_model]`\n",
    "        return x\n",
    "\n",
    "    def attn(self, layer: RelativeMultiHeadAttention, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        \"\"\"\n",
    "        This is a reimplementation of ['Multi-Head Attention'](../mha.html#MHA) which calls\n",
    "        `prepare_for_attn` instead of ['PrepareForMultiHeadAttention'](../mha.html#PrepareMHA)\n",
    "        to detach projection parameters.\n",
    "        \"\"\"\n",
    "        # Calculate query, key and value projections\n",
    "        query = self.prepare_for_attn(layer.query, query)\n",
    "        key = self.prepare_for_attn(layer.key, key)\n",
    "        value = self.prepare_for_attn(layer.value, value)\n",
    "\n",
    "        # Compute attention scores $Q K^\\top$.\n",
    "        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n",
    "        scores = torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "\n",
    "        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        scores *= layer.scale\n",
    "\n",
    "        # $softmax$ attention along the key sequence dimension\n",
    "        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = layer.softmax(scores)\n",
    "\n",
    "        # Multiply by values\n",
    "        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n",
    "        return torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "    def norm(self, ln: nn.LayerNorm, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform layer normalization with shift and scale parameters detached.\n",
    "        \"\"\"\n",
    "\n",
    "        # Detach shift(`bias`) and scaling(`weight`) parameters\n",
    "        weight = ln.weight.detach() if ln.weight is not None else None\n",
    "        bias = ln.bias.detach() if ln.bias is not None else None\n",
    "\n",
    "        # Layer normalization\n",
    "        return F.layer_norm(x, ln.normalized_shape, weight, bias, ln.eps)\n",
    "\n",
    "    def calc_loss(self, layer: CompressiveTransformerLayer, h: torch.Tensor, mem: torch.Tensor):\n",
    "        \"\"\"\n",
    "        This calculates the loss for a layer\n",
    "        \"\"\"\n",
    "\n",
    "        # Detach the token embeddings and memory.\n",
    "        h = h.detach()\n",
    "        mem = mem.detach()\n",
    "\n",
    "        # Compress the memory with $f_c^{(i)}$.\n",
    "        # The parameters of $f_c^{(i)}$ are the only parameters not detached from gradient computation.\n",
    "        c_mem = layer.compress(mem)\n",
    "\n",
    "        # Normalize the embeddings and memories\n",
    "        h = self.norm(layer.norm_self_attn, h)\n",
    "        mem = self.norm(layer.norm_self_attn, mem)\n",
    "        c_mem = self.norm(layer.norm_self_attn, c_mem)\n",
    "\n",
    "        # Calculate the attention with uncompressed memory\n",
    "        attn_mem = self.attn(layer.self_attn, h, mem, mem)\n",
    "        # Calculate the attention with compressed memory\n",
    "        attn_cmem = self.attn(layer.self_attn, h, c_mem, c_mem)\n",
    "\n",
    "        # Calculate the mean square error\n",
    "        return self.loss_func(attn_cmem, attn_mem)\n",
    "\n",
    "    def __call__(self, h: List[torch.Tensor], mem: List[torch.Tensor]):\n",
    "        # Calculate the losses for each layer\n",
    "        losses = [self.calc_loss(layer, h[n], mem[n]) for n, layer in enumerate(self.layers)]\n",
    "        # Sum of the losses\n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
